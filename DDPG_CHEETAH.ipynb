{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('HalfCheetahBulletEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(6,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  26\n",
      "Size of Action Space ->  6\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=5000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch,):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            \n",
    "            \n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    last_init=tf.random_uniform_initializer(minval=-0.003,maxval=0.003)\n",
    "    i=layers.Input(shape=(num_states))\n",
    "    x=layers.Dense(128,activation='relu',autocast=False)(i)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(num_actions,activation='tanh',kernel_initializer=last_init)(x)\n",
    "    x=x*upper_bound\n",
    "    model=tf.keras.Model(i,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=get_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 26)]              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 6)                 1542      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_mul_3 (TensorFlo [(None, 6)]               0         \n",
      "=================================================================\n",
      "Total params: 103,814\n",
      "Trainable params: 103,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic():\n",
    "    state_input=layers.Input(shape=(num_states))\n",
    "    state_output=layers.Dense(16,activation='relu',autocast=False)(state_input)\n",
    "    state_output=layers.Dense(32,activation='relu')(state_output)\n",
    "    \n",
    "    action_input=layers.Input(shape=(num_actions))\n",
    "    action_output=layers.Dense(16,activation='relu')(action_input)\n",
    "    action_output=layers.Dense(32,activation='relu')(action_output)\n",
    "\n",
    "    concat=layers.Concatenate()([state_output,action_output])\n",
    "    x=layers.Dense(256,activation='relu')(concat)\n",
    "    \n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(1,activation='linear')(x)\n",
    "    model=tf.keras.Model([state_input,action_input],x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return np.squeeze(legal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(6), std_deviation=float(std_dev) * np.ones(6))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "\n",
    "critic_lr = 0.0003\n",
    "actor_lr = 0.0003\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "tau = 0.995\n",
    "\n",
    "buffer = Buffer(100000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1468.1010549174825\n",
      "Episode * 1 * Avg Reward is ==> -1499.9516524961573\n",
      "Episode * 2 * Avg Reward is ==> -1568.015801411989\n",
      "Episode * 3 * Avg Reward is ==> -1604.3706656809459\n",
      "Episode * 4 * Avg Reward is ==> -1616.8595674273772\n",
      "Episode * 5 * Avg Reward is ==> -1625.7556427296274\n",
      "Episode * 6 * Avg Reward is ==> -1634.9470269894393\n",
      "Episode * 7 * Avg Reward is ==> -1641.5526671082325\n",
      "Episode * 8 * Avg Reward is ==> -1649.3840019440959\n",
      "Episode * 9 * Avg Reward is ==> -1653.781992989422\n",
      "Episode * 10 * Avg Reward is ==> -1655.674097555832\n",
      "Episode * 11 * Avg Reward is ==> -1658.8006146847092\n",
      "Episode * 12 * Avg Reward is ==> -1657.0087644738546\n",
      "Episode * 13 * Avg Reward is ==> -1659.443130204624\n",
      "Episode * 14 * Avg Reward is ==> -1657.4959526628543\n",
      "Episode * 15 * Avg Reward is ==> -1656.8714676412508\n",
      "Episode * 16 * Avg Reward is ==> -1652.6095093922725\n",
      "Episode * 17 * Avg Reward is ==> -1653.948974036627\n",
      "Episode * 18 * Avg Reward is ==> -1656.039871677743\n",
      "Episode * 19 * Avg Reward is ==> -1653.4908519715905\n",
      "Episode * 20 * Avg Reward is ==> -1655.6664290065344\n",
      "Episode * 21 * Avg Reward is ==> -1657.5050955789818\n",
      "Episode * 22 * Avg Reward is ==> -1657.873688541999\n",
      "Episode * 23 * Avg Reward is ==> -1659.3630582965434\n",
      "Episode * 24 * Avg Reward is ==> -1661.6234481109027\n",
      "Episode * 25 * Avg Reward is ==> -1663.7193944948344\n",
      "Episode * 26 * Avg Reward is ==> -1664.672094812886\n",
      "Episode * 27 * Avg Reward is ==> -1664.2392933776812\n",
      "Episode * 28 * Avg Reward is ==> -1664.8532406761644\n",
      "Episode * 29 * Avg Reward is ==> -1666.1067585485614\n",
      "Episode * 30 * Avg Reward is ==> -1663.041403173668\n",
      "Episode * 31 * Avg Reward is ==> -1662.203758636527\n",
      "Episode * 32 * Avg Reward is ==> -1663.5620243949818\n",
      "Episode * 33 * Avg Reward is ==> -1664.7693574403752\n",
      "Episode * 34 * Avg Reward is ==> -1666.1874566081844\n",
      "Episode * 35 * Avg Reward is ==> -1667.0830758808793\n",
      "Episode * 36 * Avg Reward is ==> -1667.7280590145683\n",
      "Episode * 37 * Avg Reward is ==> -1668.7546624520967\n",
      "Episode * 38 * Avg Reward is ==> -1669.794878328634\n",
      "Episode * 39 * Avg Reward is ==> -1670.003198835088\n",
      "Episode * 40 * Avg Reward is ==> -1670.503306967603\n",
      "Episode * 41 * Avg Reward is ==> -1670.7505914572528\n",
      "Episode * 42 * Avg Reward is ==> -1671.7382291082718\n",
      "Episode * 43 * Avg Reward is ==> -1672.325193196904\n",
      "Episode * 44 * Avg Reward is ==> -1673.1961268405669\n",
      "Episode * 45 * Avg Reward is ==> -1674.2100056073944\n",
      "Episode * 46 * Avg Reward is ==> -1672.8774546584727\n",
      "Episode * 47 * Avg Reward is ==> -1672.9874202945728\n",
      "Episode * 48 * Avg Reward is ==> -1673.5221648306226\n",
      "Episode * 49 * Avg Reward is ==> -1674.1492881466975\n",
      "Episode * 50 * Avg Reward is ==> -1679.1838747905274\n",
      "Episode * 51 * Avg Reward is ==> -1682.536443903055\n",
      "Episode * 52 * Avg Reward is ==> -1682.5642215346365\n",
      "Episode * 53 * Avg Reward is ==> -1682.1213708591415\n",
      "Episode * 54 * Avg Reward is ==> -1682.6426303867347\n",
      "Episode * 55 * Avg Reward is ==> -1683.5362020746586\n",
      "Episode * 56 * Avg Reward is ==> -1684.3902681539982\n",
      "Episode * 57 * Avg Reward is ==> -1685.1849621818508\n",
      "Episode * 58 * Avg Reward is ==> -1685.0993883449407\n",
      "Episode * 59 * Avg Reward is ==> -1685.0719369827266\n",
      "Episode * 60 * Avg Reward is ==> -1685.7426895599585\n",
      "Episode * 61 * Avg Reward is ==> -1685.9727154901234\n",
      "Episode * 62 * Avg Reward is ==> -1685.866528173867\n",
      "Episode * 63 * Avg Reward is ==> -1685.9131802137688\n",
      "Episode * 64 * Avg Reward is ==> -1687.0303274671044\n",
      "Episode * 65 * Avg Reward is ==> -1688.1781309347664\n",
      "Episode * 66 * Avg Reward is ==> -1689.7587536838912\n",
      "Episode * 67 * Avg Reward is ==> -1690.3641678829385\n",
      "Episode * 68 * Avg Reward is ==> -1690.4762189946903\n",
      "Episode * 69 * Avg Reward is ==> -1690.014107345382\n",
      "Episode * 70 * Avg Reward is ==> -1690.3477695237636\n",
      "Episode * 71 * Avg Reward is ==> -1690.6101467244907\n",
      "Episode * 72 * Avg Reward is ==> -1691.048665137596\n",
      "Episode * 73 * Avg Reward is ==> -1691.4592152009793\n",
      "Episode * 74 * Avg Reward is ==> -1691.2498782890118\n",
      "Episode * 75 * Avg Reward is ==> -1690.9467712794315\n",
      "Episode * 76 * Avg Reward is ==> -1691.1799323976031\n",
      "Episode * 77 * Avg Reward is ==> -1691.9249947712858\n",
      "Episode * 78 * Avg Reward is ==> -1692.7297103995907\n",
      "Episode * 79 * Avg Reward is ==> -1692.962951443105\n",
      "Episode * 80 * Avg Reward is ==> -1695.0606022086045\n",
      "Episode * 81 * Avg Reward is ==> -1696.3844017338022\n",
      "Episode * 82 * Avg Reward is ==> -1696.0918602567824\n",
      "Episode * 83 * Avg Reward is ==> -1695.929426939142\n",
      "Episode * 84 * Avg Reward is ==> -1695.7202061522162\n",
      "Episode * 85 * Avg Reward is ==> -1696.0944093213973\n",
      "Episode * 86 * Avg Reward is ==> -1696.5965713179987\n",
      "Episode * 87 * Avg Reward is ==> -1696.4015641586782\n",
      "Episode * 88 * Avg Reward is ==> -1696.5423738174557\n",
      "Episode * 89 * Avg Reward is ==> -1697.3816892643986\n",
      "Episode * 90 * Avg Reward is ==> -1697.6771005416952\n",
      "Episode * 91 * Avg Reward is ==> -1698.3016739325114\n",
      "Episode * 92 * Avg Reward is ==> -1698.3312443613195\n",
      "Episode * 93 * Avg Reward is ==> -1698.252132578332\n",
      "Episode * 94 * Avg Reward is ==> -1697.4085517419203\n",
      "Episode * 95 * Avg Reward is ==> -1696.7752610118528\n",
      "Episode * 96 * Avg Reward is ==> -1699.1450547584316\n",
      "Episode * 97 * Avg Reward is ==> -1699.6519492243149\n",
      "Episode * 98 * Avg Reward is ==> -1699.6748664363417\n",
      "Episode * 99 * Avg Reward is ==> -1699.6550254133463\n",
      "Episode * 100 * Avg Reward is ==> -1699.3103548432452\n",
      "Episode * 101 * Avg Reward is ==> -1699.4117249721833\n",
      "Episode * 102 * Avg Reward is ==> -1699.1910441630662\n",
      "Episode * 103 * Avg Reward is ==> -1699.3354445024534\n",
      "Episode * 104 * Avg Reward is ==> -1699.8328358332394\n",
      "Episode * 105 * Avg Reward is ==> -1699.3765031401251\n",
      "Episode * 106 * Avg Reward is ==> -1698.9942915587217\n",
      "Episode * 107 * Avg Reward is ==> -1697.7487799575426\n",
      "Episode * 108 * Avg Reward is ==> -1697.165969292726\n",
      "Episode * 109 * Avg Reward is ==> -1696.9413208175438\n",
      "Episode * 110 * Avg Reward is ==> -1695.6739557804278\n",
      "Episode * 111 * Avg Reward is ==> -1695.5451183617538\n",
      "Episode * 112 * Avg Reward is ==> -1697.0201398964598\n",
      "Episode * 113 * Avg Reward is ==> -1696.4546301716325\n",
      "Episode * 114 * Avg Reward is ==> -1694.8435232513373\n",
      "Episode * 115 * Avg Reward is ==> -1694.2185763139144\n",
      "Episode * 116 * Avg Reward is ==> -1694.8671551953737\n",
      "Episode * 117 * Avg Reward is ==> -1694.266069033795\n",
      "Episode * 118 * Avg Reward is ==> -1694.504268911775\n",
      "Episode * 119 * Avg Reward is ==> -1697.2076096817834\n",
      "Episode * 120 * Avg Reward is ==> -1696.832013261025\n",
      "Episode * 121 * Avg Reward is ==> -1696.7136174273733\n",
      "Episode * 122 * Avg Reward is ==> -1697.2521618059543\n",
      "Episode * 123 * Avg Reward is ==> -1697.3282657640382\n",
      "Episode * 124 * Avg Reward is ==> -1697.1569895243665\n",
      "Episode * 125 * Avg Reward is ==> -1696.9525047937755\n",
      "Episode * 126 * Avg Reward is ==> -1696.7203458629651\n",
      "Episode * 127 * Avg Reward is ==> -1697.1608496772233\n",
      "Episode * 128 * Avg Reward is ==> -1697.0292352373765\n",
      "Episode * 129 * Avg Reward is ==> -1696.2299248646732\n",
      "Episode * 130 * Avg Reward is ==> -1696.576458325886\n",
      "Episode * 131 * Avg Reward is ==> -1695.2847064003408\n",
      "Episode * 132 * Avg Reward is ==> -1695.050690646837\n",
      "Episode * 133 * Avg Reward is ==> -1694.3970499749005\n",
      "Episode * 134 * Avg Reward is ==> -1693.5446702879894\n",
      "Episode * 135 * Avg Reward is ==> -1693.3293863697452\n",
      "Episode * 136 * Avg Reward is ==> -1693.0112022657052\n",
      "Episode * 137 * Avg Reward is ==> -1692.5740200902944\n",
      "Episode * 138 * Avg Reward is ==> -1691.173328066785\n",
      "Episode * 139 * Avg Reward is ==> -1688.7051651898798\n",
      "Episode * 140 * Avg Reward is ==> -1688.3583290334552\n",
      "Episode * 141 * Avg Reward is ==> -1688.3126597848695\n",
      "Episode * 142 * Avg Reward is ==> -1688.523710702012\n",
      "Episode * 143 * Avg Reward is ==> -1689.2101954965751\n",
      "Episode * 144 * Avg Reward is ==> -1690.3027435535996\n",
      "Episode * 145 * Avg Reward is ==> -1690.749587229012\n",
      "Episode * 146 * Avg Reward is ==> -1690.2708567657598\n",
      "Episode * 147 * Avg Reward is ==> -1689.5315557119934\n",
      "Episode * 148 * Avg Reward is ==> -1689.3639788256464\n",
      "Episode * 149 * Avg Reward is ==> -1689.2770427328098\n",
      "Episode * 150 * Avg Reward is ==> -1688.9828482403805\n",
      "Episode * 151 * Avg Reward is ==> -1688.329412518818\n",
      "Episode * 152 * Avg Reward is ==> -1688.142071538381\n",
      "Episode * 153 * Avg Reward is ==> -1686.9522400983626\n",
      "Episode * 154 * Avg Reward is ==> -1686.4013340285296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 155 * Avg Reward is ==> -1686.6051031898596\n",
      "Episode * 156 * Avg Reward is ==> -1685.1342348874348\n",
      "Episode * 157 * Avg Reward is ==> -1685.5299507185898\n",
      "Episode * 158 * Avg Reward is ==> -1684.8369294560146\n",
      "Episode * 159 * Avg Reward is ==> -1684.0821161227102\n",
      "Episode * 160 * Avg Reward is ==> -1685.2468040699625\n",
      "Episode * 161 * Avg Reward is ==> -1685.5781981576101\n",
      "Episode * 162 * Avg Reward is ==> -1685.643486584996\n",
      "Episode * 163 * Avg Reward is ==> -1686.4324542790464\n",
      "Episode * 164 * Avg Reward is ==> -1688.7997950662873\n",
      "Episode * 165 * Avg Reward is ==> -1689.70092078485\n",
      "Episode * 166 * Avg Reward is ==> -1689.6114081660098\n",
      "Episode * 167 * Avg Reward is ==> -1689.8677112688194\n",
      "Episode * 168 * Avg Reward is ==> -1689.398615059632\n",
      "Episode * 169 * Avg Reward is ==> -1688.7478224742629\n",
      "Episode * 170 * Avg Reward is ==> -1688.5150632849266\n",
      "Episode * 171 * Avg Reward is ==> -1687.3926217374942\n",
      "Episode * 172 * Avg Reward is ==> -1686.7437927369842\n",
      "Episode * 173 * Avg Reward is ==> -1685.5030194604792\n",
      "Episode * 174 * Avg Reward is ==> -1685.2541708537435\n",
      "Episode * 175 * Avg Reward is ==> -1685.463910359863\n",
      "Episode * 176 * Avg Reward is ==> -1686.0232157506443\n",
      "Episode * 177 * Avg Reward is ==> -1685.8932521505328\n",
      "Episode * 178 * Avg Reward is ==> -1684.7497745153205\n",
      "Episode * 179 * Avg Reward is ==> -1685.1005679532611\n",
      "Episode * 180 * Avg Reward is ==> -1685.1944583634222\n",
      "Episode * 181 * Avg Reward is ==> -1686.3627861975472\n",
      "Episode * 182 * Avg Reward is ==> -1686.4579846600097\n",
      "Episode * 183 * Avg Reward is ==> -1687.2673756415286\n",
      "Episode * 184 * Avg Reward is ==> -1687.9828853236434\n",
      "Episode * 185 * Avg Reward is ==> -1687.9098838542375\n",
      "Episode * 186 * Avg Reward is ==> -1687.6883767347945\n",
      "Episode * 187 * Avg Reward is ==> -1687.6689937240808\n",
      "Episode * 188 * Avg Reward is ==> -1687.7427202773206\n",
      "Episode * 189 * Avg Reward is ==> -1689.5135385489623\n",
      "Episode * 190 * Avg Reward is ==> -1689.3442960120656\n",
      "Episode * 191 * Avg Reward is ==> -1688.649508549647\n",
      "Episode * 192 * Avg Reward is ==> -1688.2457200986244\n",
      "Episode * 193 * Avg Reward is ==> -1688.0659642952764\n",
      "Episode * 194 * Avg Reward is ==> -1687.8854285519199\n",
      "Episode * 195 * Avg Reward is ==> -1688.0190985490583\n",
      "Episode * 196 * Avg Reward is ==> -1687.7361796076439\n",
      "Episode * 197 * Avg Reward is ==> -1687.6000643108553\n",
      "Episode * 198 * Avg Reward is ==> -1687.6499170636203\n",
      "Episode * 199 * Avg Reward is ==> -1686.8203768770522\n",
      "Episode * 200 * Avg Reward is ==> -1687.6348098207945\n",
      "Episode * 201 * Avg Reward is ==> -1687.9158801809856\n",
      "Episode * 202 * Avg Reward is ==> -1687.3109196765283\n",
      "Episode * 203 * Avg Reward is ==> -1688.6280795921555\n",
      "Episode * 204 * Avg Reward is ==> -1689.1250809831606\n",
      "Episode * 205 * Avg Reward is ==> -1689.2502189489999\n",
      "Episode * 206 * Avg Reward is ==> -1690.5017640601798\n",
      "Episode * 207 * Avg Reward is ==> -1690.469709822402\n",
      "Episode * 208 * Avg Reward is ==> -1691.4653936614566\n",
      "Episode * 209 * Avg Reward is ==> -1690.6207743785258\n",
      "Episode * 210 * Avg Reward is ==> -1689.258065594041\n",
      "Episode * 211 * Avg Reward is ==> -1688.7296027677771\n",
      "Episode * 212 * Avg Reward is ==> -1688.7988156318604\n",
      "Episode * 213 * Avg Reward is ==> -1689.202101831234\n",
      "Episode * 214 * Avg Reward is ==> -1689.119398059005\n",
      "Episode * 215 * Avg Reward is ==> -1688.584077896493\n",
      "Episode * 216 * Avg Reward is ==> -1688.7697502864203\n",
      "Episode * 217 * Avg Reward is ==> -1688.3567505104086\n",
      "Episode * 218 * Avg Reward is ==> -1688.4279378909832\n",
      "Episode * 219 * Avg Reward is ==> -1689.1461502669565\n",
      "Episode * 220 * Avg Reward is ==> -1688.8629392667615\n",
      "Episode * 221 * Avg Reward is ==> -1689.3450815402746\n",
      "Episode * 222 * Avg Reward is ==> -1689.4880256813042\n",
      "Episode * 223 * Avg Reward is ==> -1690.512319837957\n",
      "Episode * 224 * Avg Reward is ==> -1691.0391592892768\n",
      "Episode * 225 * Avg Reward is ==> -1691.615672749544\n",
      "Episode * 226 * Avg Reward is ==> -1691.4921126685406\n",
      "Episode * 227 * Avg Reward is ==> -1691.230730911845\n",
      "Episode * 228 * Avg Reward is ==> -1690.7983233108014\n",
      "Episode * 229 * Avg Reward is ==> -1689.4120734915282\n",
      "Episode * 230 * Avg Reward is ==> -1688.8911008182579\n",
      "Episode * 231 * Avg Reward is ==> -1688.8539463798165\n",
      "Episode * 232 * Avg Reward is ==> -1689.5821961508261\n",
      "Episode * 233 * Avg Reward is ==> -1689.7220249531017\n",
      "Episode * 234 * Avg Reward is ==> -1689.829251375983\n",
      "Episode * 235 * Avg Reward is ==> -1689.3264463529788\n",
      "Episode * 236 * Avg Reward is ==> -1688.7418236694457\n",
      "Episode * 237 * Avg Reward is ==> -1688.8693236355832\n",
      "Episode * 238 * Avg Reward is ==> -1689.9928565193175\n",
      "Episode * 239 * Avg Reward is ==> -1690.3394349578093\n",
      "Episode * 240 * Avg Reward is ==> -1690.9691499583469\n",
      "Episode * 241 * Avg Reward is ==> -1691.350723111596\n",
      "Episode * 242 * Avg Reward is ==> -1691.5166945477622\n",
      "Episode * 243 * Avg Reward is ==> -1691.087412034854\n",
      "Episode * 244 * Avg Reward is ==> -1691.1142804166595\n",
      "Episode * 245 * Avg Reward is ==> -1691.0203925321257\n",
      "Episode * 246 * Avg Reward is ==> -1691.3340562465135\n",
      "Episode * 247 * Avg Reward is ==> -1692.314245886788\n",
      "Episode * 248 * Avg Reward is ==> -1692.3283350522772\n",
      "Episode * 249 * Avg Reward is ==> -1692.3997463401265\n",
      "Episode * 250 * Avg Reward is ==> -1691.379301435229\n",
      "Episode * 251 * Avg Reward is ==> -1690.9684650754016\n",
      "Episode * 252 * Avg Reward is ==> -1691.5172654720163\n",
      "Episode * 253 * Avg Reward is ==> -1690.83988979366\n",
      "Episode * 254 * Avg Reward is ==> -1689.9566631333532\n",
      "Episode * 255 * Avg Reward is ==> -1689.821815456987\n",
      "Episode * 256 * Avg Reward is ==> -1690.0464850265807\n",
      "Episode * 257 * Avg Reward is ==> -1690.6507263954\n",
      "Episode * 258 * Avg Reward is ==> -1691.2293203739732\n",
      "Episode * 259 * Avg Reward is ==> -1693.621986047919\n",
      "Episode * 260 * Avg Reward is ==> -1694.9420558869783\n",
      "Episode * 261 * Avg Reward is ==> -1695.2709106910813\n",
      "Episode * 262 * Avg Reward is ==> -1695.0181597249275\n",
      "Episode * 263 * Avg Reward is ==> -1694.0927704349142\n",
      "Episode * 264 * Avg Reward is ==> -1693.5007112643925\n",
      "Episode * 265 * Avg Reward is ==> -1693.6221082577401\n",
      "Episode * 266 * Avg Reward is ==> -1692.9297912862198\n",
      "Episode * 267 * Avg Reward is ==> -1693.4942822484775\n",
      "Episode * 268 * Avg Reward is ==> -1692.7605411353827\n",
      "Episode * 269 * Avg Reward is ==> -1692.2648932509596\n",
      "Episode * 270 * Avg Reward is ==> -1691.8763755264083\n",
      "Episode * 271 * Avg Reward is ==> -1692.6118030925645\n",
      "Episode * 272 * Avg Reward is ==> -1691.7814149821916\n",
      "Episode * 273 * Avg Reward is ==> -1691.3976907089889\n",
      "Episode * 274 * Avg Reward is ==> -1691.5807809206815\n",
      "Episode * 275 * Avg Reward is ==> -1691.3942227349028\n",
      "Episode * 276 * Avg Reward is ==> -1690.7305552494408\n",
      "Episode * 277 * Avg Reward is ==> -1690.0543930183992\n",
      "Episode * 278 * Avg Reward is ==> -1690.8696413190007\n",
      "Episode * 279 * Avg Reward is ==> -1692.5771907090661\n",
      "Episode * 280 * Avg Reward is ==> -1692.473764659821\n",
      "Episode * 281 * Avg Reward is ==> -1692.6225843793022\n",
      "Episode * 282 * Avg Reward is ==> -1692.2690334552492\n",
      "Episode * 283 * Avg Reward is ==> -1692.0095696684252\n",
      "Episode * 284 * Avg Reward is ==> -1692.4041982285387\n",
      "Episode * 285 * Avg Reward is ==> -1692.020967541461\n",
      "Episode * 286 * Avg Reward is ==> -1692.797948140022\n",
      "Episode * 287 * Avg Reward is ==> -1693.5534739040077\n",
      "Episode * 288 * Avg Reward is ==> -1693.6137363007156\n",
      "Episode * 289 * Avg Reward is ==> -1693.3157550821156\n",
      "Episode * 290 * Avg Reward is ==> -1693.0375838747195\n",
      "Episode * 291 * Avg Reward is ==> -1693.1964015239325\n",
      "Episode * 292 * Avg Reward is ==> -1693.2550934149713\n",
      "Episode * 293 * Avg Reward is ==> -1693.1396036108758\n",
      "Episode * 294 * Avg Reward is ==> -1693.0312474686832\n",
      "Episode * 295 * Avg Reward is ==> -1693.0573624887177\n",
      "Episode * 296 * Avg Reward is ==> -1693.1052877861769\n",
      "Episode * 297 * Avg Reward is ==> -1693.1894465263601\n",
      "Episode * 298 * Avg Reward is ==> -1693.4452990445395\n",
      "Episode * 299 * Avg Reward is ==> -1694.4361611597985\n",
      "Episode * 300 * Avg Reward is ==> -1694.3942179157382\n",
      "Episode * 301 * Avg Reward is ==> -1694.435134037059\n",
      "Episode * 302 * Avg Reward is ==> -1694.8683867943912\n",
      "Episode * 303 * Avg Reward is ==> -1694.5855304947981\n",
      "Episode * 304 * Avg Reward is ==> -1694.581214801638\n",
      "Episode * 305 * Avg Reward is ==> -1693.3833223270774\n",
      "Episode * 306 * Avg Reward is ==> -1693.3425073622204\n",
      "Episode * 307 * Avg Reward is ==> -1691.018973668005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 308 * Avg Reward is ==> -1690.1798906747833\n",
      "Episode * 309 * Avg Reward is ==> -1689.612564175578\n",
      "Episode * 310 * Avg Reward is ==> -1689.312823355564\n",
      "Episode * 311 * Avg Reward is ==> -1688.7258948108743\n",
      "Episode * 312 * Avg Reward is ==> -1688.7822623796753\n",
      "Episode * 313 * Avg Reward is ==> -1689.2185378742363\n",
      "Episode * 314 * Avg Reward is ==> -1689.0427173888331\n",
      "Episode * 315 * Avg Reward is ==> -1688.1081708570555\n",
      "Episode * 316 * Avg Reward is ==> -1688.0553960411976\n",
      "Episode * 317 * Avg Reward is ==> -1687.8377253613812\n",
      "Episode * 318 * Avg Reward is ==> -1688.5507742967536\n",
      "Episode * 319 * Avg Reward is ==> -1688.3233816336297\n",
      "Episode * 320 * Avg Reward is ==> -1688.5912598283207\n",
      "Episode * 321 * Avg Reward is ==> -1688.5273758622127\n",
      "Episode * 322 * Avg Reward is ==> -1689.785763721083\n",
      "Episode * 323 * Avg Reward is ==> -1690.4290306559349\n",
      "Episode * 324 * Avg Reward is ==> -1690.119716837912\n",
      "Episode * 325 * Avg Reward is ==> -1689.71224963905\n",
      "Episode * 326 * Avg Reward is ==> -1690.1979355007902\n",
      "Episode * 327 * Avg Reward is ==> -1691.315034708158\n",
      "Episode * 328 * Avg Reward is ==> -1691.7662294617328\n",
      "Episode * 329 * Avg Reward is ==> -1691.3818779005624\n",
      "Episode * 330 * Avg Reward is ==> -1692.2495359412321\n",
      "Episode * 331 * Avg Reward is ==> -1692.0058807616026\n",
      "Episode * 332 * Avg Reward is ==> -1691.5456929900165\n",
      "Episode * 333 * Avg Reward is ==> -1691.3033292931736\n",
      "Episode * 334 * Avg Reward is ==> -1691.0597230599474\n",
      "Episode * 335 * Avg Reward is ==> -1691.957356107842\n",
      "Episode * 336 * Avg Reward is ==> -1691.6367765942969\n",
      "Episode * 337 * Avg Reward is ==> -1691.0173544932948\n",
      "Episode * 338 * Avg Reward is ==> -1690.625024892944\n",
      "Episode * 339 * Avg Reward is ==> -1691.258784052594\n",
      "Episode * 340 * Avg Reward is ==> -1691.488663561773\n",
      "Episode * 341 * Avg Reward is ==> -1691.5975834009985\n",
      "Episode * 342 * Avg Reward is ==> -1690.9404488455734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-4c661c0007d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtf_prev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_prev_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mou_noise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mexpand_dims_v2\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m   \"\"\"\n\u001b[1;32m--> 399\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m   2187\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   2188\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ExpandDims\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2189\u001b[1;33m         tld.op_callbacks, input, axis)\n\u001b[0m\u001b[0;32m   2190\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2191\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ep_reward_list = []\n",
    "\n",
    "avg_reward_list = []\n",
    "\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "    \n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_reward = np.mean(ep_reward_list[-50:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
