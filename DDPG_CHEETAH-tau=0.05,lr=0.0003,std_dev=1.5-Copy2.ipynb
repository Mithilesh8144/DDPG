{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('HalfCheetahBulletEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(6,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  26\n",
      "Size of Action Space ->  6\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity, batch_size):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch,):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            \n",
    "            \n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    last_init=tf.random_uniform_initializer(minval=-0.003,maxval=0.003)\n",
    "    i=layers.Input(shape=(num_states))\n",
    "    x=layers.Dense(128,activation='relu',autocast=False)(i)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(num_actions,activation='tanh',kernel_initializer=last_init)(x)\n",
    "    x=x*upper_bound\n",
    "    model=tf.keras.Model(i,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=get_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 26)]              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 6)                 1542      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_mul_3 (TensorFlo [(None, 6)]               0         \n",
      "=================================================================\n",
      "Total params: 103,814\n",
      "Trainable params: 103,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic():\n",
    "    state_input=layers.Input(shape=(num_states))\n",
    "    state_output=layers.Dense(16,activation='relu',autocast=False)(state_input)\n",
    "    state_output=layers.Dense(32,activation='relu')(state_output)\n",
    "    \n",
    "    action_input=layers.Input(shape=(num_actions))\n",
    "    action_output=layers.Dense(16,activation='relu')(action_input)\n",
    "    action_output=layers.Dense(32,activation='relu')(action_output)\n",
    "\n",
    "    concat=layers.Concatenate()([state_output,action_output])\n",
    "    x=layers.Dense(256,activation='relu')(concat)\n",
    "    \n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(1,activation='linear')(x)\n",
    "    model=tf.keras.Model([state_input,action_input],x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return np.squeeze(legal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.10\n",
    "ou_noise = OUActionNoise(mean=np.zeros(6), std_deviation=float(std_dev) * np.ones(6))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "\n",
    "critic_lr = 0.0003\n",
    "actor_lr = 0.0003\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "tau = 0.05\n",
    "\n",
    "buffer = Buffer(100000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1312.38484240967\n",
      "Episode * 1 * Avg Reward is ==> -1430.4294234505905\n",
      "Episode * 2 * Avg Reward is ==> -1421.5442130707004\n",
      "Episode * 3 * Avg Reward is ==> -1384.1029479256968\n",
      "Episode * 4 * Avg Reward is ==> -1253.8300970643709\n",
      "Episode * 5 * Avg Reward is ==> -1272.2132816545688\n",
      "Episode * 6 * Avg Reward is ==> -1303.3183758872485\n",
      "Episode * 7 * Avg Reward is ==> -1333.9338061188753\n",
      "Episode * 8 * Avg Reward is ==> -1365.9740561853903\n",
      "Episode * 9 * Avg Reward is ==> -1389.2221465747973\n",
      "Episode * 10 * Avg Reward is ==> -1401.760944436606\n",
      "Episode * 11 * Avg Reward is ==> -1412.1328389355224\n",
      "Episode * 12 * Avg Reward is ==> -1427.1074105707476\n",
      "Episode * 13 * Avg Reward is ==> -1444.219894511162\n",
      "Episode * 14 * Avg Reward is ==> -1443.669390128032\n",
      "Episode * 15 * Avg Reward is ==> -1450.8962121243317\n",
      "Episode * 16 * Avg Reward is ==> -1401.5682126100778\n",
      "Episode * 17 * Avg Reward is ==> -1415.97877757893\n",
      "Episode * 18 * Avg Reward is ==> -1425.0703685497167\n",
      "Episode * 19 * Avg Reward is ==> -1417.986141500868\n",
      "Episode * 20 * Avg Reward is ==> -1414.6310351750494\n",
      "Episode * 21 * Avg Reward is ==> -1396.6674156342508\n",
      "Episode * 22 * Avg Reward is ==> -1395.6059428505155\n",
      "Episode * 23 * Avg Reward is ==> -1402.061748121818\n",
      "Episode * 24 * Avg Reward is ==> -1405.7884167486327\n",
      "Episode * 25 * Avg Reward is ==> -1330.6650890184085\n",
      "Episode * 26 * Avg Reward is ==> -1281.1849425494238\n",
      "Episode * 27 * Avg Reward is ==> -1238.9083200479777\n",
      "Episode * 28 * Avg Reward is ==> -1223.3433688128891\n",
      "Episode * 29 * Avg Reward is ==> -1232.8768707271367\n",
      "Episode * 30 * Avg Reward is ==> -1240.843416054896\n",
      "Episode * 31 * Avg Reward is ==> -1210.5300447603026\n",
      "Episode * 32 * Avg Reward is ==> -1166.2269172557033\n",
      "Episode * 33 * Avg Reward is ==> -1146.371451087475\n",
      "Episode * 34 * Avg Reward is ==> -1132.1406271439932\n",
      "Episode * 35 * Avg Reward is ==> -1130.2283289548652\n",
      "Episode * 36 * Avg Reward is ==> -1129.7445725794569\n",
      "Episode * 37 * Avg Reward is ==> -1143.7918855787052\n",
      "Episode * 38 * Avg Reward is ==> -1140.251992303281\n",
      "Episode * 39 * Avg Reward is ==> -1143.7280749954375\n",
      "Episode * 40 * Avg Reward is ==> -1128.5361329397892\n",
      "Episode * 41 * Avg Reward is ==> -1120.5586740361437\n",
      "Episode * 42 * Avg Reward is ==> -1124.704570348845\n",
      "Episode * 43 * Avg Reward is ==> -1125.5315349476118\n",
      "Episode * 44 * Avg Reward is ==> -1111.7040660261591\n",
      "Episode * 45 * Avg Reward is ==> -1089.124676625031\n",
      "Episode * 46 * Avg Reward is ==> -1081.669475671678\n",
      "Episode * 47 * Avg Reward is ==> -1057.7684862598026\n",
      "Episode * 48 * Avg Reward is ==> -1056.6735594940296\n",
      "Episode * 49 * Avg Reward is ==> -1035.446562462104\n",
      "Episode * 50 * Avg Reward is ==> -1004.5681035431066\n",
      "Episode * 51 * Avg Reward is ==> -970.8390472563302\n",
      "Episode * 52 * Avg Reward is ==> -944.5407205203858\n",
      "Episode * 53 * Avg Reward is ==> -930.282344001769\n",
      "Episode * 54 * Avg Reward is ==> -908.3598897989883\n",
      "Episode * 55 * Avg Reward is ==> -908.1145036950094\n",
      "Episode * 56 * Avg Reward is ==> -884.1246440425004\n",
      "Episode * 57 * Avg Reward is ==> -876.615178082658\n",
      "Episode * 58 * Avg Reward is ==> -868.9660449465297\n",
      "Episode * 59 * Avg Reward is ==> -857.7833793289252\n",
      "Episode * 60 * Avg Reward is ==> -842.4784543461934\n",
      "Episode * 61 * Avg Reward is ==> -813.0341916702171\n",
      "Episode * 62 * Avg Reward is ==> -773.355635403846\n",
      "Episode * 63 * Avg Reward is ==> -743.3888922591826\n",
      "Episode * 64 * Avg Reward is ==> -737.534647448716\n",
      "Episode * 65 * Avg Reward is ==> -700.6830972461377\n",
      "Episode * 66 * Avg Reward is ==> -702.8403761371754\n",
      "Episode * 67 * Avg Reward is ==> -657.3375963978755\n",
      "Episode * 68 * Avg Reward is ==> -615.427894202838\n",
      "Episode * 69 * Avg Reward is ==> -605.2735262159549\n",
      "Episode * 70 * Avg Reward is ==> -583.4029730677556\n",
      "Episode * 71 * Avg Reward is ==> -556.9189537719051\n",
      "Episode * 72 * Avg Reward is ==> -524.8386614397189\n",
      "Episode * 73 * Avg Reward is ==> -486.20798622266585\n",
      "Episode * 74 * Avg Reward is ==> -454.32423205524566\n",
      "Episode * 75 * Avg Reward is ==> -453.5296786296913\n",
      "Episode * 76 * Avg Reward is ==> -442.6845980572912\n",
      "Episode * 77 * Avg Reward is ==> -429.9474634724401\n",
      "Episode * 78 * Avg Reward is ==> -403.9838996526235\n",
      "Episode * 79 * Avg Reward is ==> -363.6245613244768\n",
      "Episode * 80 * Avg Reward is ==> -322.528386429077\n",
      "Episode * 81 * Avg Reward is ==> -310.3768339340863\n",
      "Episode * 82 * Avg Reward is ==> -314.2246375666021\n",
      "Episode * 83 * Avg Reward is ==> -305.8029952371417\n",
      "Episode * 84 * Avg Reward is ==> -298.6719637915257\n",
      "Episode * 85 * Avg Reward is ==> -268.07598978229254\n",
      "Episode * 86 * Avg Reward is ==> -234.98282978965918\n",
      "Episode * 87 * Avg Reward is ==> -190.96773866816744\n",
      "Episode * 88 * Avg Reward is ==> -158.98068623448057\n",
      "Episode * 89 * Avg Reward is ==> -134.98228850123073\n",
      "Episode * 90 * Avg Reward is ==> -136.1251927530093\n",
      "Episode * 91 * Avg Reward is ==> -120.05626429985072\n",
      "Episode * 92 * Avg Reward is ==> -95.84388175550566\n",
      "Episode * 93 * Avg Reward is ==> -63.66903430662097\n",
      "Episode * 94 * Avg Reward is ==> -47.50295579791059\n",
      "Episode * 95 * Avg Reward is ==> -51.451887329419776\n",
      "Episode * 96 * Avg Reward is ==> -36.33256808467678\n",
      "Episode * 97 * Avg Reward is ==> -54.435345105293024\n",
      "Episode * 98 * Avg Reward is ==> -61.402402740205474\n",
      "Episode * 99 * Avg Reward is ==> -72.38397942200382\n",
      "Episode * 100 * Avg Reward is ==> -71.07804293227399\n",
      "Episode * 101 * Avg Reward is ==> -90.01285556188394\n",
      "Episode * 102 * Avg Reward is ==> -91.7219190712519\n",
      "Episode * 103 * Avg Reward is ==> -81.32805933135789\n",
      "Episode * 104 * Avg Reward is ==> -81.03995402243346\n",
      "Episode * 105 * Avg Reward is ==> -56.52951496848788\n",
      "Episode * 106 * Avg Reward is ==> -54.43448939867329\n",
      "Episode * 107 * Avg Reward is ==> -22.0212723336553\n",
      "Episode * 108 * Avg Reward is ==> 1.805272037787425\n",
      "Episode * 109 * Avg Reward is ==> 15.261207103151174\n",
      "Episode * 110 * Avg Reward is ==> 40.95320636875263\n",
      "Episode * 111 * Avg Reward is ==> 48.729601728863564\n",
      "Episode * 112 * Avg Reward is ==> 43.973036310040925\n",
      "Episode * 113 * Avg Reward is ==> 57.4950543658034\n",
      "Episode * 114 * Avg Reward is ==> 89.92777772945064\n",
      "Episode * 115 * Avg Reward is ==> 93.30688555507935\n",
      "Episode * 116 * Avg Reward is ==> 117.55087747429019\n",
      "Episode * 117 * Avg Reward is ==> 109.54989529262697\n",
      "Episode * 118 * Avg Reward is ==> 108.61388638046628\n",
      "Episode * 119 * Avg Reward is ==> 127.6306085946733\n",
      "Episode * 120 * Avg Reward is ==> 123.04323042065724\n",
      "Episode * 121 * Avg Reward is ==> 125.11005848526034\n",
      "Episode * 122 * Avg Reward is ==> 125.8796381410114\n",
      "Episode * 123 * Avg Reward is ==> 117.96486811078331\n",
      "Episode * 124 * Avg Reward is ==> 124.39630032625551\n",
      "Episode * 125 * Avg Reward is ==> 122.54110059237995\n",
      "Episode * 126 * Avg Reward is ==> 120.33074065322712\n",
      "Episode * 127 * Avg Reward is ==> 119.32035806491855\n",
      "Episode * 128 * Avg Reward is ==> 114.79852556846558\n",
      "Episode * 129 * Avg Reward is ==> 113.76658157351567\n",
      "Episode * 130 * Avg Reward is ==> 111.08897041291952\n",
      "Episode * 131 * Avg Reward is ==> 114.24452299346112\n",
      "Episode * 132 * Avg Reward is ==> 123.36080896740167\n",
      "Episode * 133 * Avg Reward is ==> 134.36418014322552\n",
      "Episode * 134 * Avg Reward is ==> 144.1627284633658\n",
      "Episode * 135 * Avg Reward is ==> 144.95297224770312\n",
      "Episode * 136 * Avg Reward is ==> 145.00405665006303\n",
      "Episode * 137 * Avg Reward is ==> 142.5348103505042\n",
      "Episode * 138 * Avg Reward is ==> 140.25351050510892\n",
      "Episode * 139 * Avg Reward is ==> 151.47452333853013\n",
      "Episode * 140 * Avg Reward is ==> 172.52822287406073\n",
      "Episode * 141 * Avg Reward is ==> 182.67401325701175\n",
      "Episode * 142 * Avg Reward is ==> 195.2125044712526\n",
      "Episode * 143 * Avg Reward is ==> 195.27147335658861\n",
      "Episode * 144 * Avg Reward is ==> 189.7611890393502\n",
      "Episode * 145 * Avg Reward is ==> 206.06930126174942\n",
      "Episode * 146 * Avg Reward is ==> 209.112206379993\n",
      "Episode * 147 * Avg Reward is ==> 235.27438479861706\n",
      "Episode * 148 * Avg Reward is ==> 274.2653858752448\n",
      "Episode * 149 * Avg Reward is ==> 295.13266836122034\n",
      "Episode * 150 * Avg Reward is ==> 300.1313745670239\n",
      "Episode * 151 * Avg Reward is ==> 323.18006459318576\n",
      "Episode * 152 * Avg Reward is ==> 331.85033301113054\n",
      "Episode * 153 * Avg Reward is ==> 344.6416132745256\n",
      "Episode * 154 * Avg Reward is ==> 346.06764448012814\n",
      "Episode * 155 * Avg Reward is ==> 358.47810235221584\n",
      "Episode * 156 * Avg Reward is ==> 370.45057598931356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 157 * Avg Reward is ==> 371.3758612302956\n",
      "Episode * 158 * Avg Reward is ==> 383.7611211047625\n",
      "Episode * 159 * Avg Reward is ==> 402.5281211543713\n",
      "Episode * 160 * Avg Reward is ==> 390.92280734815574\n",
      "Episode * 161 * Avg Reward is ==> 388.57886430803643\n",
      "Episode * 162 * Avg Reward is ==> 388.71510519517915\n",
      "Episode * 163 * Avg Reward is ==> 380.4276628438992\n",
      "Episode * 164 * Avg Reward is ==> 380.89448082659413\n",
      "Episode * 165 * Avg Reward is ==> 380.2389678691693\n",
      "Episode * 166 * Avg Reward is ==> 363.80505929111837\n",
      "Episode * 167 * Avg Reward is ==> 331.8473206331401\n",
      "Episode * 168 * Avg Reward is ==> 330.9382719971882\n",
      "Episode * 169 * Avg Reward is ==> 332.98799459816917\n",
      "Episode * 170 * Avg Reward is ==> 351.86950096266804\n",
      "Episode * 171 * Avg Reward is ==> 355.4238279812261\n",
      "Episode * 172 * Avg Reward is ==> 362.65714723387663\n",
      "Episode * 173 * Avg Reward is ==> 374.82293162766763\n",
      "Episode * 174 * Avg Reward is ==> 379.631925065757\n",
      "Episode * 175 * Avg Reward is ==> 381.135413257346\n",
      "Episode * 176 * Avg Reward is ==> 383.4560472251876\n",
      "Episode * 177 * Avg Reward is ==> 385.63086303928503\n",
      "Episode * 178 * Avg Reward is ==> 391.1329791051214\n",
      "Episode * 179 * Avg Reward is ==> 395.47936650310766\n",
      "Episode * 180 * Avg Reward is ==> 400.00805201244873\n",
      "Episode * 181 * Avg Reward is ==> 403.48274611938473\n",
      "Episode * 182 * Avg Reward is ==> 405.832889648591\n",
      "Episode * 183 * Avg Reward is ==> 405.80640884659283\n",
      "Episode * 184 * Avg Reward is ==> 394.8163387482197\n",
      "Episode * 185 * Avg Reward is ==> 397.684162527248\n",
      "Episode * 186 * Avg Reward is ==> 397.2883000159172\n",
      "Episode * 187 * Avg Reward is ==> 387.0507787165451\n",
      "Episode * 188 * Avg Reward is ==> 380.6695068129955\n",
      "Episode * 189 * Avg Reward is ==> 381.89073265772873\n",
      "Episode * 190 * Avg Reward is ==> 361.12010594096546\n",
      "Episode * 191 * Avg Reward is ==> 358.8890490827572\n",
      "Episode * 192 * Avg Reward is ==> 358.7826823723125\n",
      "Episode * 193 * Avg Reward is ==> 361.2697412943598\n",
      "Episode * 194 * Avg Reward is ==> 373.59125548975635\n",
      "Episode * 195 * Avg Reward is ==> 367.9647092194073\n",
      "Episode * 196 * Avg Reward is ==> 377.53939530206435\n",
      "Episode * 197 * Avg Reward is ==> 381.22233828155214\n",
      "Episode * 198 * Avg Reward is ==> 381.05265832155897\n",
      "Episode * 199 * Avg Reward is ==> 383.0528205541939\n",
      "Episode * 200 * Avg Reward is ==> 381.66572001625724\n",
      "Episode * 201 * Avg Reward is ==> 386.28554737863374\n",
      "Episode * 202 * Avg Reward is ==> 391.34583869443514\n",
      "Episode * 203 * Avg Reward is ==> 390.64838600596033\n",
      "Episode * 204 * Avg Reward is ==> 392.02265637714146\n",
      "Episode * 205 * Avg Reward is ==> 394.76249952825015\n",
      "Episode * 206 * Avg Reward is ==> 398.7786690353129\n",
      "Episode * 207 * Avg Reward is ==> 399.6028372594299\n",
      "Episode * 208 * Avg Reward is ==> 398.48170505532767\n",
      "Episode * 209 * Avg Reward is ==> 398.1564759729627\n",
      "Episode * 210 * Avg Reward is ==> 408.3947706449027\n",
      "Episode * 211 * Avg Reward is ==> 413.58929981976735\n",
      "Episode * 212 * Avg Reward is ==> 418.6792442256907\n",
      "Episode * 213 * Avg Reward is ==> 421.896399972266\n",
      "Episode * 214 * Avg Reward is ==> 418.28709183873184\n",
      "Episode * 215 * Avg Reward is ==> 407.3460832703265\n",
      "Episode * 216 * Avg Reward is ==> 422.12639839058136\n",
      "Episode * 217 * Avg Reward is ==> 445.9243043942978\n",
      "Episode * 218 * Avg Reward is ==> 445.4899427933684\n",
      "Episode * 219 * Avg Reward is ==> 426.67615236332\n",
      "Episode * 220 * Avg Reward is ==> 428.6060354855546\n",
      "Episode * 221 * Avg Reward is ==> 428.2746119555816\n",
      "Episode * 222 * Avg Reward is ==> 428.8523211310396\n",
      "Episode * 223 * Avg Reward is ==> 428.0079677701043\n",
      "Episode * 224 * Avg Reward is ==> 425.6184320761208\n",
      "Episode * 225 * Avg Reward is ==> 425.3715172605865\n",
      "Episode * 226 * Avg Reward is ==> 421.898188555323\n",
      "Episode * 227 * Avg Reward is ==> 421.81623909723754\n",
      "Episode * 228 * Avg Reward is ==> 423.8966407507512\n",
      "Episode * 229 * Avg Reward is ==> 422.88943073737596\n",
      "Episode * 230 * Avg Reward is ==> 420.2326801580313\n",
      "Episode * 231 * Avg Reward is ==> 417.71336220206416\n",
      "Episode * 232 * Avg Reward is ==> 416.9013262816658\n",
      "Episode * 233 * Avg Reward is ==> 417.5764109154253\n",
      "Episode * 234 * Avg Reward is ==> 436.3991844284708\n",
      "Episode * 235 * Avg Reward is ==> 433.8008826994431\n",
      "Episode * 236 * Avg Reward is ==> 434.98774030381713\n",
      "Episode * 237 * Avg Reward is ==> 444.86435547242536\n",
      "Episode * 238 * Avg Reward is ==> 447.7166736318906\n",
      "Episode * 239 * Avg Reward is ==> 450.2842767377663\n",
      "Episode * 240 * Avg Reward is ==> 471.495184387995\n",
      "Episode * 241 * Avg Reward is ==> 475.98901477728157\n",
      "Episode * 242 * Avg Reward is ==> 476.10368174011876\n",
      "Episode * 243 * Avg Reward is ==> 477.65364445504326\n",
      "Episode * 244 * Avg Reward is ==> 477.5652890033345\n",
      "Episode * 245 * Avg Reward is ==> 484.4963380214652\n",
      "Episode * 246 * Avg Reward is ==> 479.11288707229977\n",
      "Episode * 247 * Avg Reward is ==> 476.9500351334834\n",
      "Episode * 248 * Avg Reward is ==> 477.2004773306625\n",
      "Episode * 249 * Avg Reward is ==> 478.01638116090953\n",
      "Episode * 250 * Avg Reward is ==> 481.58048860692156\n",
      "Episode * 251 * Avg Reward is ==> 483.1454057474122\n",
      "Episode * 252 * Avg Reward is ==> 486.25696272857226\n",
      "Episode * 253 * Avg Reward is ==> 486.2385171874931\n",
      "Episode * 254 * Avg Reward is ==> 487.1097796621097\n",
      "Episode * 255 * Avg Reward is ==> 482.177453279181\n",
      "Episode * 256 * Avg Reward is ==> 480.24444866418287\n",
      "Episode * 257 * Avg Reward is ==> 482.0210216512948\n",
      "Episode * 258 * Avg Reward is ==> 484.4251261319844\n",
      "Episode * 259 * Avg Reward is ==> 487.0903210614971\n",
      "Episode * 260 * Avg Reward is ==> 488.8516451395558\n",
      "Episode * 261 * Avg Reward is ==> 487.4068309552838\n",
      "Episode * 262 * Avg Reward is ==> 489.70517497526225\n",
      "Episode * 263 * Avg Reward is ==> 497.9524203703965\n",
      "Episode * 264 * Avg Reward is ==> 504.7593030458499\n",
      "Episode * 265 * Avg Reward is ==> 520.0936475364578\n",
      "Episode * 266 * Avg Reward is ==> 524.1194061147118\n",
      "Episode * 267 * Avg Reward is ==> 538.1310962958722\n",
      "Episode * 268 * Avg Reward is ==> 543.1483754519181\n",
      "Episode * 269 * Avg Reward is ==> 569.0837690133598\n",
      "Episode * 270 * Avg Reward is ==> 572.2433624221663\n",
      "Episode * 271 * Avg Reward is ==> 567.1237095054674\n",
      "Episode * 272 * Avg Reward is ==> 557.3984581859055\n",
      "Episode * 273 * Avg Reward is ==> 560.2005748091071\n",
      "Episode * 274 * Avg Reward is ==> 560.7923559210967\n",
      "Episode * 275 * Avg Reward is ==> 563.1590724983824\n",
      "Episode * 276 * Avg Reward is ==> 566.1353369651974\n",
      "Episode * 277 * Avg Reward is ==> 566.4468051380155\n",
      "Episode * 278 * Avg Reward is ==> 567.3016442302115\n",
      "Episode * 279 * Avg Reward is ==> 570.0187393219958\n",
      "Episode * 280 * Avg Reward is ==> 554.2302502314687\n",
      "Episode * 281 * Avg Reward is ==> 555.5949903584075\n",
      "Episode * 282 * Avg Reward is ==> 555.0587777385999\n",
      "Episode * 283 * Avg Reward is ==> 557.4093965713672\n",
      "Episode * 284 * Avg Reward is ==> 559.4439869904211\n",
      "Episode * 285 * Avg Reward is ==> 559.026214965117\n",
      "Episode * 286 * Avg Reward is ==> 557.0873464614313\n",
      "Episode * 287 * Avg Reward is ==> 555.440145237781\n",
      "Episode * 288 * Avg Reward is ==> 561.0774289815806\n",
      "Episode * 289 * Avg Reward is ==> 558.2993831637475\n",
      "Episode * 290 * Avg Reward is ==> 560.923527830261\n",
      "Episode * 291 * Avg Reward is ==> 559.3855376612348\n",
      "Episode * 292 * Avg Reward is ==> 559.6586678869039\n",
      "Episode * 293 * Avg Reward is ==> 558.9225569146387\n",
      "Episode * 294 * Avg Reward is ==> 560.3438987479218\n",
      "Episode * 295 * Avg Reward is ==> 561.97762847298\n",
      "Episode * 296 * Avg Reward is ==> 558.7271699985089\n",
      "Episode * 297 * Avg Reward is ==> 566.550472857009\n",
      "Episode * 298 * Avg Reward is ==> 569.5867832849801\n",
      "Episode * 299 * Avg Reward is ==> 568.8202886911226\n",
      "Episode * 300 * Avg Reward is ==> 569.6559827009386\n",
      "Episode * 301 * Avg Reward is ==> 549.080964752767\n",
      "Episode * 302 * Avg Reward is ==> 548.7779123665786\n",
      "Episode * 303 * Avg Reward is ==> 555.976244722767\n",
      "Episode * 304 * Avg Reward is ==> 559.6521890948302\n",
      "Episode * 305 * Avg Reward is ==> 567.0830649494601\n",
      "Episode * 306 * Avg Reward is ==> 572.821533466302\n",
      "Episode * 307 * Avg Reward is ==> 572.9435308997977\n",
      "Episode * 308 * Avg Reward is ==> 573.0492112107834\n",
      "Episode * 309 * Avg Reward is ==> 569.5725824578899\n",
      "Episode * 310 * Avg Reward is ==> 577.4588456539963\n",
      "Episode * 311 * Avg Reward is ==> 584.2035993138047\n",
      "Episode * 312 * Avg Reward is ==> 590.0596177000646\n",
      "Episode * 313 * Avg Reward is ==> 592.6075741115213\n",
      "Episode * 314 * Avg Reward is ==> 594.8120472369092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 315 * Avg Reward is ==> 589.5072574545841\n",
      "Episode * 316 * Avg Reward is ==> 590.9258918576105\n",
      "Episode * 317 * Avg Reward is ==> 591.2912191136352\n",
      "Episode * 318 * Avg Reward is ==> 596.6174452327118\n",
      "Episode * 319 * Avg Reward is ==> 602.5289549767849\n",
      "Episode * 320 * Avg Reward is ==> 606.4189101373372\n",
      "Episode * 321 * Avg Reward is ==> 616.8538943101572\n",
      "Episode * 322 * Avg Reward is ==> 630.1630375630168\n",
      "Episode * 323 * Avg Reward is ==> 638.0425980479196\n",
      "Episode * 324 * Avg Reward is ==> 647.1788597336517\n",
      "Episode * 325 * Avg Reward is ==> 653.5910113992146\n",
      "Episode * 326 * Avg Reward is ==> 665.5855020284527\n",
      "Episode * 327 * Avg Reward is ==> 674.6682536905948\n",
      "Episode * 328 * Avg Reward is ==> 678.4260578547037\n",
      "Episode * 329 * Avg Reward is ==> 686.0394570862002\n",
      "Episode * 330 * Avg Reward is ==> 713.55061875256\n",
      "Episode * 331 * Avg Reward is ==> 721.6166482453939\n",
      "Episode * 332 * Avg Reward is ==> 721.5953409513417\n",
      "Episode * 333 * Avg Reward is ==> 728.3540755535078\n",
      "Episode * 334 * Avg Reward is ==> 735.5061358060353\n",
      "Episode * 335 * Avg Reward is ==> 741.1130880594573\n",
      "Episode * 336 * Avg Reward is ==> 752.4460316539366\n",
      "Episode * 337 * Avg Reward is ==> 758.0833464764708\n",
      "Episode * 338 * Avg Reward is ==> 768.3597333857454\n",
      "Episode * 339 * Avg Reward is ==> 776.0992193173199\n",
      "Episode * 340 * Avg Reward is ==> 781.8893019425318\n",
      "Episode * 341 * Avg Reward is ==> 791.9264029301302\n",
      "Episode * 342 * Avg Reward is ==> 800.3565296585869\n",
      "Episode * 343 * Avg Reward is ==> 810.9614301379889\n",
      "Episode * 344 * Avg Reward is ==> 818.2790735091635\n",
      "Episode * 345 * Avg Reward is ==> 822.5483778884493\n",
      "Episode * 346 * Avg Reward is ==> 843.0524338099137\n",
      "Episode * 347 * Avg Reward is ==> 849.0627745684325\n",
      "Episode * 348 * Avg Reward is ==> 861.3973211106379\n",
      "Episode * 349 * Avg Reward is ==> 870.6473493448829\n",
      "Episode * 350 * Avg Reward is ==> 878.3993890605883\n",
      "Episode * 351 * Avg Reward is ==> 908.5980287488615\n",
      "Episode * 352 * Avg Reward is ==> 909.2117698543481\n",
      "Episode * 353 * Avg Reward is ==> 919.9579710715038\n",
      "Episode * 354 * Avg Reward is ==> 921.8688228216465\n",
      "Episode * 355 * Avg Reward is ==> 932.3014868615892\n",
      "Episode * 356 * Avg Reward is ==> 933.2522917316168\n",
      "Episode * 357 * Avg Reward is ==> 905.4494984779781\n",
      "Episode * 358 * Avg Reward is ==> 915.593667255663\n",
      "Episode * 359 * Avg Reward is ==> 925.0586263935386\n",
      "Episode * 360 * Avg Reward is ==> 927.6053660664968\n",
      "Episode * 361 * Avg Reward is ==> 933.5296369119983\n",
      "Episode * 362 * Avg Reward is ==> 939.0190980776965\n",
      "Episode * 363 * Avg Reward is ==> 946.2015058063276\n",
      "Episode * 364 * Avg Reward is ==> 956.3511114510795\n",
      "Episode * 365 * Avg Reward is ==> 973.4070208270407\n",
      "Episode * 366 * Avg Reward is ==> 988.473591813734\n",
      "Episode * 367 * Avg Reward is ==> 1007.7806983454956\n",
      "Episode * 368 * Avg Reward is ==> 1019.9109018664644\n",
      "Episode * 369 * Avg Reward is ==> 1031.4514889514155\n",
      "Episode * 370 * Avg Reward is ==> 1036.072780310692\n",
      "Episode * 371 * Avg Reward is ==> 1048.4774765402776\n",
      "Episode * 372 * Avg Reward is ==> 1062.4466342987682\n",
      "Episode * 373 * Avg Reward is ==> 1071.8820572086952\n",
      "Episode * 374 * Avg Reward is ==> 1082.965151758863\n",
      "Episode * 375 * Avg Reward is ==> 1093.7523170859288\n",
      "Episode * 376 * Avg Reward is ==> 1101.1488809749972\n",
      "Episode * 377 * Avg Reward is ==> 1106.3421620158354\n",
      "Episode * 378 * Avg Reward is ==> 1117.4176127682665\n",
      "Episode * 379 * Avg Reward is ==> 1122.6909247586577\n",
      "Episode * 380 * Avg Reward is ==> 1127.4739188705726\n",
      "Episode * 381 * Avg Reward is ==> 1134.457814175734\n",
      "Episode * 382 * Avg Reward is ==> 1151.5752665004945\n",
      "Episode * 383 * Avg Reward is ==> 1161.076158438233\n",
      "Episode * 384 * Avg Reward is ==> 1169.763883698976\n",
      "Episode * 385 * Avg Reward is ==> 1184.3404822427246\n"
     ]
    }
   ],
   "source": [
    "ep_reward_list = []\n",
    "\n",
    "avg_reward_list = []\n",
    "\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "    \n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_reward = np.mean(ep_reward_list[-50:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_episodes=[]\n",
    "for i in range(0,500):\n",
    "    no_episodes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(no_episodes,ep_reward_list, color = 'red',  linestyle = '-', \n",
    "        label = 'Total Reward0')\n",
    "\n",
    "plt.plot(no_episodes, avg_reward_list, color = 'midnightblue',  linestyle = '--', \n",
    "        label = 'Episodes Reward Average0')\n",
    "\n",
    "\n",
    "plt.grid(b = True, which = 'major', axis = 'y', linestyle = '--')\n",
    "plt.xlabel('Episode', fontsize = 12)\n",
    "plt.ylabel('Reward',  fontsize = 12)\n",
    "plt.title('Total Reward per Testing Episode',  fontsize = 12)  \n",
    "plt.legend(loc = 'lower right', fontsize = 12)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model.save('Cheetah_actor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_model.save('Cheetah_critic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 500 * Avg Reward is ==> 1318.1714488483956\n",
      "Episode * 501 * Avg Reward is ==> 1320.5825065772174\n",
      "Episode * 502 * Avg Reward is ==> 1323.1363426276882\n",
      "Episode * 503 * Avg Reward is ==> 1324.914377464224\n",
      "Episode * 504 * Avg Reward is ==> 1327.3868113680908\n",
      "Episode * 505 * Avg Reward is ==> 1330.720752027385\n",
      "Episode * 506 * Avg Reward is ==> 1316.9254810235311\n",
      "Episode * 507 * Avg Reward is ==> 1326.0672774441032\n",
      "Episode * 508 * Avg Reward is ==> 1353.6581605197055\n",
      "Episode * 509 * Avg Reward is ==> 1348.9286780839875\n",
      "Episode * 510 * Avg Reward is ==> 1351.3389527383574\n",
      "Episode * 511 * Avg Reward is ==> 1360.4409699190148\n",
      "Episode * 512 * Avg Reward is ==> 1362.369846114073\n"
     ]
    }
   ],
   "source": [
    "for ep in range(500,700):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "    \n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_reward = np.mean(ep_reward_list[-50:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
