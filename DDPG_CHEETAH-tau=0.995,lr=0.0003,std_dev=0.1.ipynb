{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('HalfCheetahBulletEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(6,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  26\n",
      "Size of Action Space ->  6\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_noise = OUActionNoise(mean=np.zeros(6), std_deviation=float(0.2) * np.ones(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=ou_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00871219, -0.0261643 ,  0.00355288,  0.00560832, -0.01063394,\n",
       "       -0.04180368])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Guassian:\n",
    "    def __init__(self,std_deviation):\n",
    "        self.std_dev=std_deviation\n",
    "    def __call__(self):\n",
    "        return np.random.normal(loc=0,scale=self.std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=Guassian(std_deviation=float(0.2)*np.ones(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise1=noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01067067,  0.08721855,  0.0576646 ,  0.02838344, -0.05643014,\n",
       "        0.05374886])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=5000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch,):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            \n",
    "            \n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    #last_init=tf.random_uniform_initializer(minval=-0.003,maxval=0.003)\n",
    "    i=layers.Input(shape=(num_states))\n",
    "    x=layers.Dense(128,activation='relu',autocast=False)(i)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(num_actions,activation='tanh')(x)\n",
    "    x=x*upper_bound\n",
    "    model=tf.keras.Model(i,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=get_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 26)]              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 6)                 1542      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_mul_3 (TensorFlo [(None, 6)]               0         \n",
      "=================================================================\n",
      "Total params: 103,814\n",
      "Trainable params: 103,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic():\n",
    "    state_input=layers.Input(shape=(num_states))\n",
    "    state_output=layers.Dense(16,activation='relu',autocast=False)(state_input)\n",
    "    state_output=layers.Dense(32,activation='relu')(state_output)\n",
    "    \n",
    "    action_input=layers.Input(shape=(num_actions))\n",
    "    action_output=layers.Dense(16,activation='relu')(action_input)\n",
    "    action_output=layers.Dense(32,activation='relu')(action_output)\n",
    "\n",
    "    concat=layers.Concatenate()([state_output,action_output])\n",
    "    x=layers.Dense(256,activation='relu')(concat)\n",
    "    \n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(1,activation='linear')(x)\n",
    "    model=tf.keras.Model([state_input,action_input],x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return np.squeeze(legal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.15\n",
    "ou_noise = OUActionNoise(mean=np.zeros(6), std_deviation=float(std_dev) * np.ones(6))\n",
    "#noise=Guassian(std_deviation=float(std_dev)*np.ones(6))\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "\n",
    "critic_lr = 0.0003\n",
    "actor_lr = 0.0003\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "tau = 0.995\n",
    "\n",
    "buffer = Buffer(100000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1594.918929221344\n",
      "Episode * 1 * Avg Reward is ==> -1629.7943576880512\n",
      "Episode * 2 * Avg Reward is ==> -1637.843624861387\n",
      "Episode * 3 * Avg Reward is ==> -1645.9862889113665\n",
      "Episode * 4 * Avg Reward is ==> -1648.4813796907667\n",
      "Episode * 5 * Avg Reward is ==> -1650.8947857384337\n",
      "Episode * 6 * Avg Reward is ==> -1651.4696031240069\n",
      "Episode * 7 * Avg Reward is ==> -1650.3533686390542\n",
      "Episode * 8 * Avg Reward is ==> -1650.6769207431446\n",
      "Episode * 9 * Avg Reward is ==> -1651.3116598635156\n",
      "Episode * 10 * Avg Reward is ==> -1653.1362502080044\n",
      "Episode * 11 * Avg Reward is ==> -1653.5581721090184\n",
      "Episode * 12 * Avg Reward is ==> -1653.1706335660797\n",
      "Episode * 13 * Avg Reward is ==> -1651.4543003503313\n",
      "Episode * 14 * Avg Reward is ==> -1651.3901312143012\n",
      "Episode * 15 * Avg Reward is ==> -1651.3395139115796\n",
      "Episode * 16 * Avg Reward is ==> -1652.4015431336659\n",
      "Episode * 17 * Avg Reward is ==> -1652.8821342693782\n",
      "Episode * 18 * Avg Reward is ==> -1653.210072470588\n",
      "Episode * 19 * Avg Reward is ==> -1653.0298128627953\n",
      "Episode * 20 * Avg Reward is ==> -1653.9918881810254\n",
      "Episode * 21 * Avg Reward is ==> -1653.5497433868334\n",
      "Episode * 22 * Avg Reward is ==> -1653.5671085201425\n",
      "Episode * 23 * Avg Reward is ==> -1653.5915287729094\n",
      "Episode * 24 * Avg Reward is ==> -1653.6692608821738\n",
      "Episode * 25 * Avg Reward is ==> -1653.6385660566173\n",
      "Episode * 26 * Avg Reward is ==> -1653.2041591236584\n",
      "Episode * 27 * Avg Reward is ==> -1653.7068906302848\n",
      "Episode * 28 * Avg Reward is ==> -1654.2028846707037\n",
      "Episode * 29 * Avg Reward is ==> -1654.922677222697\n",
      "Episode * 30 * Avg Reward is ==> -1655.5671099132219\n",
      "Episode * 31 * Avg Reward is ==> -1654.349176785578\n",
      "Episode * 32 * Avg Reward is ==> -1654.459071185524\n",
      "Episode * 33 * Avg Reward is ==> -1654.1791289869018\n",
      "Episode * 34 * Avg Reward is ==> -1652.8965023287224\n",
      "Episode * 35 * Avg Reward is ==> -1653.0023902706298\n",
      "Episode * 36 * Avg Reward is ==> -1653.0523624903128\n",
      "Episode * 37 * Avg Reward is ==> -1653.7965439458128\n",
      "Episode * 38 * Avg Reward is ==> -1654.0226551567785\n",
      "Episode * 39 * Avg Reward is ==> -1654.3057615139267\n",
      "Episode * 40 * Avg Reward is ==> -1654.859187195518\n",
      "Episode * 41 * Avg Reward is ==> -1655.227563361241\n",
      "Episode * 42 * Avg Reward is ==> -1655.0914095194012\n",
      "Episode * 43 * Avg Reward is ==> -1654.6530108115812\n",
      "Episode * 44 * Avg Reward is ==> -1654.4254808602486\n",
      "Episode * 45 * Avg Reward is ==> -1654.5106935737674\n",
      "Episode * 46 * Avg Reward is ==> -1654.8305578661295\n",
      "Episode * 47 * Avg Reward is ==> -1655.1010283533758\n",
      "Episode * 48 * Avg Reward is ==> -1655.3936782092842\n",
      "Episode * 49 * Avg Reward is ==> -1655.511028331531\n",
      "Episode * 50 * Avg Reward is ==> -1656.788430159405\n",
      "Episode * 51 * Avg Reward is ==> -1656.1427885333806\n",
      "Episode * 52 * Avg Reward is ==> -1656.1720637544918\n",
      "Episode * 53 * Avg Reward is ==> -1655.874377906853\n",
      "Episode * 54 * Avg Reward is ==> -1656.2090786576696\n",
      "Episode * 55 * Avg Reward is ==> -1656.187566500243\n",
      "Episode * 56 * Avg Reward is ==> -1656.0761777079883\n",
      "Episode * 57 * Avg Reward is ==> -1656.2760926056758\n",
      "Episode * 58 * Avg Reward is ==> -1656.5179203180755\n",
      "Episode * 59 * Avg Reward is ==> -1656.7072168276259\n",
      "Episode * 60 * Avg Reward is ==> -1656.265504463082\n",
      "Episode * 61 * Avg Reward is ==> -1656.3101614516709\n",
      "Episode * 62 * Avg Reward is ==> -1656.9263784612099\n",
      "Episode * 63 * Avg Reward is ==> -1657.5874679248006\n",
      "Episode * 64 * Avg Reward is ==> -1657.380545675372\n",
      "Episode * 65 * Avg Reward is ==> -1656.9165000102203\n",
      "Episode * 66 * Avg Reward is ==> -1657.0704665424546\n",
      "Episode * 67 * Avg Reward is ==> -1657.6287752718124\n",
      "Episode * 68 * Avg Reward is ==> -1657.9591162054128\n",
      "Episode * 69 * Avg Reward is ==> -1658.1253597317657\n",
      "Episode * 70 * Avg Reward is ==> -1657.4919558987153\n",
      "Episode * 71 * Avg Reward is ==> -1657.7525624373368\n",
      "Episode * 72 * Avg Reward is ==> -1658.0893820613046\n",
      "Episode * 73 * Avg Reward is ==> -1657.9085357396418\n",
      "Episode * 74 * Avg Reward is ==> -1657.7911731047411\n",
      "Episode * 75 * Avg Reward is ==> -1657.0341690471175\n",
      "Episode * 76 * Avg Reward is ==> -1657.4424746355064\n",
      "Episode * 77 * Avg Reward is ==> -1655.8429839348228\n",
      "Episode * 78 * Avg Reward is ==> -1655.3499851251147\n",
      "Episode * 79 * Avg Reward is ==> -1654.9514310949778\n",
      "Episode * 80 * Avg Reward is ==> -1654.8405494281096\n",
      "Episode * 81 * Avg Reward is ==> -1655.3206431527467\n",
      "Episode * 82 * Avg Reward is ==> -1655.2683430146199\n",
      "Episode * 83 * Avg Reward is ==> -1654.956519984472\n",
      "Episode * 84 * Avg Reward is ==> -1655.8363757552454\n",
      "Episode * 85 * Avg Reward is ==> -1655.9198321128754\n",
      "Episode * 86 * Avg Reward is ==> -1655.9530330910043\n",
      "Episode * 87 * Avg Reward is ==> -1655.3746906971514\n",
      "Episode * 88 * Avg Reward is ==> -1655.5114228087282\n",
      "Episode * 89 * Avg Reward is ==> -1655.5953869337152\n",
      "Episode * 90 * Avg Reward is ==> -1655.3023131101293\n",
      "Episode * 91 * Avg Reward is ==> -1655.3969075109937\n",
      "Episode * 92 * Avg Reward is ==> -1655.783190619686\n",
      "Episode * 93 * Avg Reward is ==> -1656.3640961426308\n",
      "Episode * 94 * Avg Reward is ==> -1656.5854381051477\n",
      "Episode * 95 * Avg Reward is ==> -1655.3207796473382\n",
      "Episode * 96 * Avg Reward is ==> -1654.9374938144604\n",
      "Episode * 97 * Avg Reward is ==> -1654.753565142328\n",
      "Episode * 98 * Avg Reward is ==> -1654.2240182222\n",
      "Episode * 99 * Avg Reward is ==> -1654.4686562703748\n",
      "Episode * 100 * Avg Reward is ==> -1654.681858074309\n",
      "Episode * 101 * Avg Reward is ==> -1654.3515054812162\n",
      "Episode * 102 * Avg Reward is ==> -1654.6645634753286\n",
      "Episode * 103 * Avg Reward is ==> -1654.8433621319814\n",
      "Episode * 104 * Avg Reward is ==> -1654.758759179128\n",
      "Episode * 105 * Avg Reward is ==> -1654.523942339375\n",
      "Episode * 106 * Avg Reward is ==> -1654.6806629178297\n",
      "Episode * 107 * Avg Reward is ==> -1655.0214406211535\n",
      "Episode * 108 * Avg Reward is ==> -1654.9139903625683\n",
      "Episode * 109 * Avg Reward is ==> -1654.8667893909567\n",
      "Episode * 110 * Avg Reward is ==> -1655.2517382674043\n",
      "Episode * 111 * Avg Reward is ==> -1655.2842718435027\n",
      "Episode * 112 * Avg Reward is ==> -1654.7324366174332\n",
      "Episode * 113 * Avg Reward is ==> -1653.6095406560864\n",
      "Episode * 114 * Avg Reward is ==> -1654.2343789234055\n",
      "Episode * 115 * Avg Reward is ==> -1654.8812104602691\n",
      "Episode * 116 * Avg Reward is ==> -1654.578647413018\n",
      "Episode * 117 * Avg Reward is ==> -1654.1135950147802\n",
      "Episode * 118 * Avg Reward is ==> -1653.880531614897\n",
      "Episode * 119 * Avg Reward is ==> -1653.98975015375\n",
      "Episode * 120 * Avg Reward is ==> -1653.6054200577032\n",
      "Episode * 121 * Avg Reward is ==> -1652.635670390525\n",
      "Episode * 122 * Avg Reward is ==> -1652.356712276284\n",
      "Episode * 123 * Avg Reward is ==> -1652.37112791346\n",
      "Episode * 124 * Avg Reward is ==> -1652.5020423129695\n",
      "Episode * 125 * Avg Reward is ==> -1653.0151686556262\n",
      "Episode * 126 * Avg Reward is ==> -1652.8126001359917\n",
      "Episode * 127 * Avg Reward is ==> -1654.2585051458962\n",
      "Episode * 128 * Avg Reward is ==> -1654.6657502949085\n",
      "Episode * 129 * Avg Reward is ==> -1654.5576711799822\n",
      "Episode * 130 * Avg Reward is ==> -1654.3886063169696\n",
      "Episode * 131 * Avg Reward is ==> -1654.8061699433645\n",
      "Episode * 132 * Avg Reward is ==> -1654.7553771059977\n",
      "Episode * 133 * Avg Reward is ==> -1655.6891236912381\n",
      "Episode * 134 * Avg Reward is ==> -1655.5804888578052\n",
      "Episode * 135 * Avg Reward is ==> -1655.5797758317315\n",
      "Episode * 136 * Avg Reward is ==> -1655.8454553919134\n",
      "Episode * 137 * Avg Reward is ==> -1656.051174854844\n",
      "Episode * 138 * Avg Reward is ==> -1655.991446727044\n",
      "Episode * 139 * Avg Reward is ==> -1655.5430692418115\n",
      "Episode * 140 * Avg Reward is ==> -1655.8227958143384\n",
      "Episode * 141 * Avg Reward is ==> -1655.5171424330567\n",
      "Episode * 142 * Avg Reward is ==> -1655.2889847556937\n",
      "Episode * 143 * Avg Reward is ==> -1655.4558503354679\n",
      "Episode * 144 * Avg Reward is ==> -1655.3221170319564\n",
      "Episode * 145 * Avg Reward is ==> -1656.3789130391212\n",
      "Episode * 146 * Avg Reward is ==> -1656.5880397306073\n",
      "Episode * 147 * Avg Reward is ==> -1654.8916238405175\n",
      "Episode * 148 * Avg Reward is ==> -1654.59138803144\n",
      "Episode * 149 * Avg Reward is ==> -1654.1479355027366\n",
      "Episode * 150 * Avg Reward is ==> -1653.772038073919\n",
      "Episode * 151 * Avg Reward is ==> -1655.004540128461\n",
      "Episode * 152 * Avg Reward is ==> -1654.9248291321599\n",
      "Episode * 153 * Avg Reward is ==> -1654.8748598765246\n",
      "Episode * 154 * Avg Reward is ==> -1654.4812997398444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 155 * Avg Reward is ==> -1654.3981157222934\n",
      "Episode * 156 * Avg Reward is ==> -1654.0814340096342\n",
      "Episode * 157 * Avg Reward is ==> -1653.4209759806545\n",
      "Episode * 158 * Avg Reward is ==> -1651.310328811222\n",
      "Episode * 159 * Avg Reward is ==> -1650.7752393399965\n",
      "Episode * 160 * Avg Reward is ==> -1650.1778092202685\n",
      "Episode * 161 * Avg Reward is ==> -1647.250976590395\n",
      "Episode * 162 * Avg Reward is ==> -1646.8613405425742\n",
      "Episode * 163 * Avg Reward is ==> -1647.6263635976238\n",
      "Episode * 164 * Avg Reward is ==> -1647.0660936750471\n",
      "Episode * 165 * Avg Reward is ==> -1646.9144588393624\n",
      "Episode * 166 * Avg Reward is ==> -1646.9063924333448\n",
      "Episode * 167 * Avg Reward is ==> -1646.7167560429268\n",
      "Episode * 168 * Avg Reward is ==> -1646.5115822191829\n",
      "Episode * 169 * Avg Reward is ==> -1646.77339977398\n",
      "Episode * 170 * Avg Reward is ==> -1647.678191161188\n",
      "Episode * 171 * Avg Reward is ==> -1648.8299327737038\n",
      "Episode * 172 * Avg Reward is ==> -1648.7834344698608\n",
      "Episode * 173 * Avg Reward is ==> -1648.673850281025\n",
      "Episode * 174 * Avg Reward is ==> -1648.7893556868041\n",
      "Episode * 175 * Avg Reward is ==> -1649.5564998833088\n",
      "Episode * 176 * Avg Reward is ==> -1649.877173497985\n",
      "Episode * 177 * Avg Reward is ==> -1650.0441474496783\n",
      "Episode * 178 * Avg Reward is ==> -1650.1256525561466\n",
      "Episode * 179 * Avg Reward is ==> -1650.3956462885208\n",
      "Episode * 180 * Avg Reward is ==> -1650.2446858231403\n",
      "Episode * 181 * Avg Reward is ==> -1649.8073897963225\n",
      "Episode * 182 * Avg Reward is ==> -1650.0742493131465\n",
      "Episode * 183 * Avg Reward is ==> -1649.526362528113\n",
      "Episode * 184 * Avg Reward is ==> -1649.7402295504808\n",
      "Episode * 185 * Avg Reward is ==> -1649.6340080703208\n",
      "Episode * 186 * Avg Reward is ==> -1649.3001117835631\n",
      "Episode * 187 * Avg Reward is ==> -1649.275063845432\n",
      "Episode * 188 * Avg Reward is ==> -1649.3882558556752\n",
      "Episode * 189 * Avg Reward is ==> -1649.6545977207932\n",
      "Episode * 190 * Avg Reward is ==> -1649.0990963151112\n",
      "Episode * 191 * Avg Reward is ==> -1648.7736991832405\n",
      "Episode * 192 * Avg Reward is ==> -1648.4126269051444\n",
      "Episode * 193 * Avg Reward is ==> -1648.2790891371124\n",
      "Episode * 194 * Avg Reward is ==> -1648.4688720977304\n",
      "Episode * 195 * Avg Reward is ==> -1648.835318158718\n",
      "Episode * 196 * Avg Reward is ==> -1648.9177840869195\n",
      "Episode * 197 * Avg Reward is ==> -1650.690675356181\n",
      "Episode * 198 * Avg Reward is ==> -1651.5882085912654\n",
      "Episode * 199 * Avg Reward is ==> -1651.301966612599\n",
      "Episode * 200 * Avg Reward is ==> -1651.0812212761264\n",
      "Episode * 201 * Avg Reward is ==> -1650.5364604955028\n",
      "Episode * 202 * Avg Reward is ==> -1650.463734249214\n",
      "Episode * 203 * Avg Reward is ==> -1650.4208964437132\n",
      "Episode * 204 * Avg Reward is ==> -1650.2516796500454\n",
      "Episode * 205 * Avg Reward is ==> -1650.550734309986\n",
      "Episode * 206 * Avg Reward is ==> -1650.6696623592381\n",
      "Episode * 207 * Avg Reward is ==> -1650.5442366147565\n",
      "Episode * 208 * Avg Reward is ==> -1652.2300013363797\n",
      "Episode * 209 * Avg Reward is ==> -1652.137994705237\n",
      "Episode * 210 * Avg Reward is ==> -1651.6838527458056\n",
      "Episode * 211 * Avg Reward is ==> -1653.477199401791\n",
      "Episode * 212 * Avg Reward is ==> -1654.0999062896017\n",
      "Episode * 213 * Avg Reward is ==> -1654.4733461757562\n",
      "Episode * 214 * Avg Reward is ==> -1655.181557348233\n",
      "Episode * 215 * Avg Reward is ==> -1655.5603773595606\n",
      "Episode * 216 * Avg Reward is ==> -1655.7459788246042\n",
      "Episode * 217 * Avg Reward is ==> -1656.4483847485435\n",
      "Episode * 218 * Avg Reward is ==> -1656.7006334561784\n",
      "Episode * 219 * Avg Reward is ==> -1656.1307342136797\n",
      "Episode * 220 * Avg Reward is ==> -1655.4983035871383\n",
      "Episode * 221 * Avg Reward is ==> -1655.4006200576082\n",
      "Episode * 222 * Avg Reward is ==> -1655.908934875965\n",
      "Episode * 223 * Avg Reward is ==> -1656.3018987800976\n",
      "Episode * 224 * Avg Reward is ==> -1656.3600834231447\n",
      "Episode * 225 * Avg Reward is ==> -1655.9719061356245\n",
      "Episode * 226 * Avg Reward is ==> -1655.747280065602\n",
      "Episode * 227 * Avg Reward is ==> -1655.4292296312633\n",
      "Episode * 228 * Avg Reward is ==> -1655.0913671346682\n",
      "Episode * 229 * Avg Reward is ==> -1655.1159531386193\n",
      "Episode * 230 * Avg Reward is ==> -1655.242718409932\n",
      "Episode * 231 * Avg Reward is ==> -1655.6166134781959\n",
      "Episode * 232 * Avg Reward is ==> -1655.4853542848002\n",
      "Episode * 233 * Avg Reward is ==> -1655.82977541261\n",
      "Episode * 234 * Avg Reward is ==> -1655.8519347468289\n",
      "Episode * 235 * Avg Reward is ==> -1655.9899640014783\n",
      "Episode * 236 * Avg Reward is ==> -1656.1733741285786\n",
      "Episode * 237 * Avg Reward is ==> -1655.8952602010793\n",
      "Episode * 238 * Avg Reward is ==> -1655.4086040705909\n",
      "Episode * 239 * Avg Reward is ==> -1655.062086693796\n",
      "Episode * 240 * Avg Reward is ==> -1655.2517769166268\n",
      "Episode * 241 * Avg Reward is ==> -1655.6849680088321\n",
      "Episode * 242 * Avg Reward is ==> -1656.0659197868895\n",
      "Episode * 243 * Avg Reward is ==> -1654.9200087134468\n",
      "Episode * 244 * Avg Reward is ==> -1654.917999019967\n",
      "Episode * 245 * Avg Reward is ==> -1654.7194532883843\n",
      "Episode * 246 * Avg Reward is ==> -1654.6613990066062\n",
      "Episode * 247 * Avg Reward is ==> -1654.5400387874674\n",
      "Episode * 248 * Avg Reward is ==> -1654.4207164928628\n",
      "Episode * 249 * Avg Reward is ==> -1654.8364949212537\n",
      "Episode * 250 * Avg Reward is ==> -1654.8557998857161\n",
      "Episode * 251 * Avg Reward is ==> -1655.1347750331238\n",
      "Episode * 252 * Avg Reward is ==> -1655.088451859928\n",
      "Episode * 253 * Avg Reward is ==> -1654.8220417238426\n",
      "Episode * 254 * Avg Reward is ==> -1655.2292136971237\n",
      "Episode * 255 * Avg Reward is ==> -1655.2444304758583\n",
      "Episode * 256 * Avg Reward is ==> -1655.0054238232897\n",
      "Episode * 257 * Avg Reward is ==> -1655.5894210866315\n",
      "Episode * 258 * Avg Reward is ==> -1656.1101167247994\n",
      "Episode * 259 * Avg Reward is ==> -1656.9752025975902\n",
      "Episode * 260 * Avg Reward is ==> -1657.8395388655147\n",
      "Episode * 261 * Avg Reward is ==> -1659.1342922055906\n",
      "Episode * 262 * Avg Reward is ==> -1658.9158613003692\n",
      "Episode * 263 * Avg Reward is ==> -1659.0944493317327\n",
      "Episode * 264 * Avg Reward is ==> -1658.6640178505172\n",
      "Episode * 265 * Avg Reward is ==> -1658.1645772533886\n",
      "Episode * 266 * Avg Reward is ==> -1656.3428053332264\n",
      "Episode * 267 * Avg Reward is ==> -1655.0156342126252\n",
      "Episode * 268 * Avg Reward is ==> -1654.8728639456028\n",
      "Episode * 269 * Avg Reward is ==> -1655.0023972855788\n",
      "Episode * 270 * Avg Reward is ==> -1654.7464059975111\n",
      "Episode * 271 * Avg Reward is ==> -1653.5459867417653\n",
      "Episode * 272 * Avg Reward is ==> -1653.215997208655\n",
      "Episode * 273 * Avg Reward is ==> -1653.4198579619792\n",
      "Episode * 274 * Avg Reward is ==> -1653.4738153595185\n",
      "Episode * 275 * Avg Reward is ==> -1653.632172656885\n",
      "Episode * 276 * Avg Reward is ==> -1653.4347036709246\n",
      "Episode * 277 * Avg Reward is ==> -1653.6776608218686\n",
      "Episode * 278 * Avg Reward is ==> -1653.7346265824485\n",
      "Episode * 279 * Avg Reward is ==> -1654.0185365854552\n",
      "Episode * 280 * Avg Reward is ==> -1653.78027455107\n",
      "Episode * 281 * Avg Reward is ==> -1654.156201391169\n",
      "Episode * 282 * Avg Reward is ==> -1654.3520262065867\n",
      "Episode * 283 * Avg Reward is ==> -1654.27531138334\n",
      "Episode * 284 * Avg Reward is ==> -1654.3128822261103\n",
      "Episode * 285 * Avg Reward is ==> -1653.978408054887\n",
      "Episode * 286 * Avg Reward is ==> -1653.3827356170743\n",
      "Episode * 287 * Avg Reward is ==> -1652.9762451209983\n",
      "Episode * 288 * Avg Reward is ==> -1652.790632630656\n",
      "Episode * 289 * Avg Reward is ==> -1652.7194536854104\n",
      "Episode * 290 * Avg Reward is ==> -1652.849856309654\n",
      "Episode * 291 * Avg Reward is ==> -1652.4422652156518\n",
      "Episode * 292 * Avg Reward is ==> -1652.2604970102134\n",
      "Episode * 293 * Avg Reward is ==> -1653.3268008660862\n",
      "Episode * 294 * Avg Reward is ==> -1653.5119858728788\n",
      "Episode * 295 * Avg Reward is ==> -1653.093606167378\n",
      "Episode * 296 * Avg Reward is ==> -1652.931880070216\n",
      "Episode * 297 * Avg Reward is ==> -1653.0801122706769\n",
      "Episode * 298 * Avg Reward is ==> -1653.185868022073\n",
      "Episode * 299 * Avg Reward is ==> -1653.4567682175593\n",
      "Episode * 300 * Avg Reward is ==> -1653.8551747389442\n",
      "Episode * 301 * Avg Reward is ==> -1653.7619333799512\n",
      "Episode * 302 * Avg Reward is ==> -1653.6741071814363\n",
      "Episode * 303 * Avg Reward is ==> -1654.2537395609181\n",
      "Episode * 304 * Avg Reward is ==> -1654.4509823502012\n",
      "Episode * 305 * Avg Reward is ==> -1654.5173482275504\n",
      "Episode * 306 * Avg Reward is ==> -1655.0911283348466\n",
      "Episode * 307 * Avg Reward is ==> -1654.668501676847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 308 * Avg Reward is ==> -1654.5052406916484\n",
      "Episode * 309 * Avg Reward is ==> -1654.3390633677784\n",
      "Episode * 310 * Avg Reward is ==> -1654.7865340325263\n",
      "Episode * 311 * Avg Reward is ==> -1654.964676037362\n",
      "Episode * 312 * Avg Reward is ==> -1655.1288939274734\n",
      "Episode * 313 * Avg Reward is ==> -1654.7734833384673\n",
      "Episode * 314 * Avg Reward is ==> -1654.4844356406009\n",
      "Episode * 315 * Avg Reward is ==> -1654.4116400305459\n",
      "Episode * 316 * Avg Reward is ==> -1655.828208703081\n",
      "Episode * 317 * Avg Reward is ==> -1655.851706769812\n",
      "Episode * 318 * Avg Reward is ==> -1655.238634403552\n",
      "Episode * 319 * Avg Reward is ==> -1655.142125859554\n",
      "Episode * 320 * Avg Reward is ==> -1655.853221849132\n",
      "Episode * 321 * Avg Reward is ==> -1656.900907389672\n",
      "Episode * 322 * Avg Reward is ==> -1656.6403863974497\n",
      "Episode * 323 * Avg Reward is ==> -1654.8983867458248\n",
      "Episode * 324 * Avg Reward is ==> -1654.4048901540054\n",
      "Episode * 325 * Avg Reward is ==> -1654.1917407828573\n",
      "Episode * 326 * Avg Reward is ==> -1654.3536823998047\n",
      "Episode * 327 * Avg Reward is ==> -1654.101145829974\n",
      "Episode * 328 * Avg Reward is ==> -1654.033326786898\n",
      "Episode * 329 * Avg Reward is ==> -1653.2378025372789\n",
      "Episode * 330 * Avg Reward is ==> -1653.580107703087\n",
      "Episode * 331 * Avg Reward is ==> -1653.1166342199658\n",
      "Episode * 332 * Avg Reward is ==> -1652.6173083899132\n",
      "Episode * 333 * Avg Reward is ==> -1652.6790912826782\n",
      "Episode * 334 * Avg Reward is ==> -1652.7920257164224\n",
      "Episode * 335 * Avg Reward is ==> -1651.448210559727\n",
      "Episode * 336 * Avg Reward is ==> -1651.1771296084376\n",
      "Episode * 337 * Avg Reward is ==> -1651.0857734201172\n",
      "Episode * 338 * Avg Reward is ==> -1651.2036630445443\n",
      "Episode * 339 * Avg Reward is ==> -1651.4113136057404\n",
      "Episode * 340 * Avg Reward is ==> -1651.3998730284352\n",
      "Episode * 341 * Avg Reward is ==> -1651.2323434814398\n",
      "Episode * 342 * Avg Reward is ==> -1651.3463557477382\n",
      "Episode * 343 * Avg Reward is ==> -1651.1750553996044\n",
      "Episode * 344 * Avg Reward is ==> -1650.8654155108682\n",
      "Episode * 345 * Avg Reward is ==> -1651.5901222444602\n",
      "Episode * 346 * Avg Reward is ==> -1651.3706496351756\n",
      "Episode * 347 * Avg Reward is ==> -1651.0529049353554\n",
      "Episode * 348 * Avg Reward is ==> -1650.7126035873991\n",
      "Episode * 349 * Avg Reward is ==> -1650.8272058422394\n",
      "Episode * 350 * Avg Reward is ==> -1650.9354767014822\n",
      "Episode * 351 * Avg Reward is ==> -1650.9246817826463\n",
      "Episode * 352 * Avg Reward is ==> -1650.5440047376126\n",
      "Episode * 353 * Avg Reward is ==> -1649.7151351220132\n",
      "Episode * 354 * Avg Reward is ==> -1649.2528584537204\n",
      "Episode * 355 * Avg Reward is ==> -1648.2553284824928\n",
      "Episode * 356 * Avg Reward is ==> -1647.1920170963767\n",
      "Episode * 357 * Avg Reward is ==> -1647.5873279979426\n",
      "Episode * 358 * Avg Reward is ==> -1647.8727356875993\n",
      "Episode * 359 * Avg Reward is ==> -1647.2809028524923\n",
      "Episode * 360 * Avg Reward is ==> -1646.762998241981\n",
      "Episode * 361 * Avg Reward is ==> -1646.254709442219\n",
      "Episode * 362 * Avg Reward is ==> -1646.4578797828174\n",
      "Episode * 363 * Avg Reward is ==> -1646.5246452409376\n",
      "Episode * 364 * Avg Reward is ==> -1646.7483105923043\n",
      "Episode * 365 * Avg Reward is ==> -1646.5204273291147\n",
      "Episode * 366 * Avg Reward is ==> -1645.6424007330215\n",
      "Episode * 367 * Avg Reward is ==> -1646.575880575363\n",
      "Episode * 368 * Avg Reward is ==> -1647.1638860240598\n",
      "Episode * 369 * Avg Reward is ==> -1647.1206975837965\n",
      "Episode * 370 * Avg Reward is ==> -1646.97019025528\n",
      "Episode * 371 * Avg Reward is ==> -1647.0374905604863\n",
      "Episode * 372 * Avg Reward is ==> -1647.2007716749586\n",
      "Episode * 373 * Avg Reward is ==> -1648.5656760269665\n",
      "Episode * 374 * Avg Reward is ==> -1648.8525420974129\n",
      "Episode * 375 * Avg Reward is ==> -1648.8429265664909\n",
      "Episode * 376 * Avg Reward is ==> -1649.1097466340786\n",
      "Episode * 377 * Avg Reward is ==> -1649.3665082598918\n",
      "Episode * 378 * Avg Reward is ==> -1649.7139121195912\n",
      "Episode * 379 * Avg Reward is ==> -1650.124878631027\n",
      "Episode * 380 * Avg Reward is ==> -1649.6000943406384\n",
      "Episode * 381 * Avg Reward is ==> -1649.3033307413746\n",
      "Episode * 382 * Avg Reward is ==> -1649.4833426945313\n",
      "Episode * 383 * Avg Reward is ==> -1648.9302646856265\n",
      "Episode * 384 * Avg Reward is ==> -1648.4695938948262\n",
      "Episode * 385 * Avg Reward is ==> -1649.7919824377686\n",
      "Episode * 386 * Avg Reward is ==> -1650.5572559658144\n",
      "Episode * 387 * Avg Reward is ==> -1651.5819642764282\n",
      "Episode * 388 * Avg Reward is ==> -1652.0943798172384\n",
      "Episode * 389 * Avg Reward is ==> -1652.7574478891793\n",
      "Episode * 390 * Avg Reward is ==> -1652.7474079711546\n",
      "Episode * 391 * Avg Reward is ==> -1653.3477315145271\n",
      "Episode * 392 * Avg Reward is ==> -1653.5517240412848\n",
      "Episode * 393 * Avg Reward is ==> -1653.6825591818517\n",
      "Episode * 394 * Avg Reward is ==> -1654.2503427358276\n",
      "Episode * 395 * Avg Reward is ==> -1653.3327709169546\n",
      "Episode * 396 * Avg Reward is ==> -1653.8219876046153\n",
      "Episode * 397 * Avg Reward is ==> -1654.4186937714755\n",
      "Episode * 398 * Avg Reward is ==> -1654.7311685896564\n",
      "Episode * 399 * Avg Reward is ==> -1654.8498368937142\n",
      "Episode * 400 * Avg Reward is ==> -1654.6523132746106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-4c661c0007d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mepisodic_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-914607f0a4e6>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-914607f0a4e6>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, state_batch, action_batch, reward_batch, next_state_batch)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtarget_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_actions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mcritic_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcritic_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    715\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m    716\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5603\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5604\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5605\u001b[1;33m         transpose_b)\n\u001b[0m\u001b[0;32m   5606\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5607\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ep_reward_list = []\n",
    "\n",
    "avg_reward_list = []\n",
    "\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "    \n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_reward = np.mean(ep_reward_list[-50:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
