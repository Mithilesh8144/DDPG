{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('HalfCheetahBulletEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(6,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  26\n",
      "Size of Action Space ->  6\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_noise = OUActionNoise(mean=np.zeros(6), std_deviation=float(0.2) * np.ones(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=ou_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02096969, -0.02391916,  0.00753846,  0.00948452,  0.03040835,\n",
       "       -0.02833086])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Guassian:\n",
    "    def __init__(self,std_deviation):\n",
    "        self.std_dev=std_deviation\n",
    "    def __call__(self):\n",
    "        return np.random.normal(loc=0,scale=self.std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=Guassian(std_deviation=float(0.2)*np.ones(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise1=noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39239268,  0.14174764, -0.03194223,  0.24984545,  0.17259738,\n",
       "       -0.08709279])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=5000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch,):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            \n",
    "            \n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    #last_init=tf.random_uniform_initializer(minval=-0.003,maxval=0.003)\n",
    "    i=layers.Input(shape=(num_states))\n",
    "    x=layers.Dense(128,activation='relu',autocast=False)(i)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(num_actions,activation='tanh')(x)\n",
    "    x=x*upper_bound\n",
    "    model=tf.keras.Model(i,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=get_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 26)]              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 6)                 1542      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_mul_5 (TensorFlo [(None, 6)]               0         \n",
      "=================================================================\n",
      "Total params: 103,814\n",
      "Trainable params: 103,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic():\n",
    "    state_input=layers.Input(shape=(num_states))\n",
    "    state_output=layers.Dense(16,activation='relu',autocast=False)(state_input)\n",
    "    state_output=layers.Dense(32,activation='relu')(state_output)\n",
    "    \n",
    "    action_input=layers.Input(shape=(num_actions))\n",
    "    action_output=layers.Dense(16,activation='relu')(action_input)\n",
    "    action_output=layers.Dense(32,activation='relu')(action_output)\n",
    "\n",
    "    concat=layers.Concatenate()([state_output,action_output])\n",
    "    x=layers.Dense(256,activation='relu')(concat)\n",
    "    \n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(1,activation='linear')(x)\n",
    "    model=tf.keras.Model([state_input,action_input],x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return np.squeeze(legal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "#ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "noise=Guassian(std_deviation=float(std_dev)*np.ones(6))\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "\n",
    "critic_lr = 0.0003\n",
    "actor_lr = 0.0003\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "tau = 0.995\n",
    "\n",
    "buffer = Buffer(100000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1336.0656114446515\n",
      "Episode * 1 * Avg Reward is ==> -1500.6111300433272\n",
      "Episode * 2 * Avg Reward is ==> -1551.0551264662772\n",
      "Episode * 3 * Avg Reward is ==> -1573.7522280514904\n",
      "Episode * 4 * Avg Reward is ==> -1593.0662326064867\n",
      "Episode * 5 * Avg Reward is ==> -1606.0666675042091\n",
      "Episode * 6 * Avg Reward is ==> -1617.0681191375313\n",
      "Episode * 7 * Avg Reward is ==> -1623.4028733914752\n",
      "Episode * 8 * Avg Reward is ==> -1628.3943465497641\n",
      "Episode * 9 * Avg Reward is ==> -1633.923240397168\n",
      "Episode * 10 * Avg Reward is ==> -1637.188994911274\n",
      "Episode * 11 * Avg Reward is ==> -1637.9749989764985\n",
      "Episode * 12 * Avg Reward is ==> -1639.8743394788287\n",
      "Episode * 13 * Avg Reward is ==> -1642.0963133511564\n",
      "Episode * 14 * Avg Reward is ==> -1643.1570745058025\n",
      "Episode * 15 * Avg Reward is ==> -1644.2159027587784\n",
      "Episode * 16 * Avg Reward is ==> -1646.2237856752242\n",
      "Episode * 17 * Avg Reward is ==> -1648.8024185352272\n",
      "Episode * 18 * Avg Reward is ==> -1650.3970715165294\n",
      "Episode * 19 * Avg Reward is ==> -1651.6573047397512\n",
      "Episode * 20 * Avg Reward is ==> -1650.8279571854575\n",
      "Episode * 21 * Avg Reward is ==> -1651.6993861428928\n",
      "Episode * 22 * Avg Reward is ==> -1651.9136406354978\n",
      "Episode * 23 * Avg Reward is ==> -1653.1809251810184\n",
      "Episode * 24 * Avg Reward is ==> -1654.4785548231996\n",
      "Episode * 25 * Avg Reward is ==> -1655.6003538322175\n",
      "Episode * 26 * Avg Reward is ==> -1656.631376403703\n",
      "Episode * 27 * Avg Reward is ==> -1657.0632733177877\n",
      "Episode * 28 * Avg Reward is ==> -1656.5784236050076\n",
      "Episode * 29 * Avg Reward is ==> -1656.7383301781238\n",
      "Episode * 30 * Avg Reward is ==> -1657.1061565888033\n",
      "Episode * 31 * Avg Reward is ==> -1657.5861826169898\n",
      "Episode * 32 * Avg Reward is ==> -1657.9424497881987\n",
      "Episode * 33 * Avg Reward is ==> -1658.3901058989493\n",
      "Episode * 34 * Avg Reward is ==> -1658.926479970191\n",
      "Episode * 35 * Avg Reward is ==> -1658.8807015591012\n",
      "Episode * 36 * Avg Reward is ==> -1659.6687459619823\n",
      "Episode * 37 * Avg Reward is ==> -1660.0341241489734\n",
      "Episode * 38 * Avg Reward is ==> -1660.0800349284486\n",
      "Episode * 39 * Avg Reward is ==> -1660.6952984485329\n",
      "Episode * 40 * Avg Reward is ==> -1661.3097359702706\n",
      "Episode * 41 * Avg Reward is ==> -1661.480273795492\n",
      "Episode * 42 * Avg Reward is ==> -1661.9150495844626\n",
      "Episode * 43 * Avg Reward is ==> -1662.4339718995573\n",
      "Episode * 44 * Avg Reward is ==> -1662.4325224422516\n",
      "Episode * 45 * Avg Reward is ==> -1662.4410955268336\n",
      "Episode * 46 * Avg Reward is ==> -1662.5126065960505\n",
      "Episode * 47 * Avg Reward is ==> -1662.9522892011244\n",
      "Episode * 48 * Avg Reward is ==> -1662.9058269572697\n",
      "Episode * 49 * Avg Reward is ==> -1663.3480582285308\n",
      "Episode * 50 * Avg Reward is ==> -1669.7445541837171\n",
      "Episode * 51 * Avg Reward is ==> -1669.8276725686692\n",
      "Episode * 52 * Avg Reward is ==> -1669.8337215884326\n",
      "Episode * 53 * Avg Reward is ==> -1669.2974765287352\n",
      "Episode * 54 * Avg Reward is ==> -1669.332741393793\n",
      "Episode * 55 * Avg Reward is ==> -1668.9878598075184\n",
      "Episode * 56 * Avg Reward is ==> -1668.0689682404945\n",
      "Episode * 57 * Avg Reward is ==> -1667.765124208901\n",
      "Episode * 58 * Avg Reward is ==> -1667.4925724313307\n",
      "Episode * 59 * Avg Reward is ==> -1667.4280450135327\n",
      "Episode * 60 * Avg Reward is ==> -1667.3929748422704\n",
      "Episode * 61 * Avg Reward is ==> -1668.2507006079097\n",
      "Episode * 62 * Avg Reward is ==> -1668.2449292552521\n",
      "Episode * 63 * Avg Reward is ==> -1667.531881567619\n",
      "Episode * 64 * Avg Reward is ==> -1668.0044646372728\n",
      "Episode * 65 * Avg Reward is ==> -1667.9185664260501\n",
      "Episode * 66 * Avg Reward is ==> -1667.8647662929359\n",
      "Episode * 67 * Avg Reward is ==> -1667.7449891047152\n",
      "Episode * 68 * Avg Reward is ==> -1667.3812438188152\n",
      "Episode * 69 * Avg Reward is ==> -1666.99311297148\n",
      "Episode * 70 * Avg Reward is ==> -1667.713852031222\n",
      "Episode * 71 * Avg Reward is ==> -1666.9185581177883\n",
      "Episode * 72 * Avg Reward is ==> -1667.005786643951\n",
      "Episode * 73 * Avg Reward is ==> -1666.7464872167131\n",
      "Episode * 74 * Avg Reward is ==> -1666.3682355123337\n",
      "Episode * 75 * Avg Reward is ==> -1665.9154587151365\n",
      "Episode * 76 * Avg Reward is ==> -1665.639795491633\n",
      "Episode * 77 * Avg Reward is ==> -1665.6655287582646\n",
      "Episode * 78 * Avg Reward is ==> -1666.2869246498908\n",
      "Episode * 79 * Avg Reward is ==> -1666.6261335583151\n",
      "Episode * 80 * Avg Reward is ==> -1666.9288655201196\n",
      "Episode * 81 * Avg Reward is ==> -1666.6213537019096\n",
      "Episode * 82 * Avg Reward is ==> -1666.9338516217417\n",
      "Episode * 83 * Avg Reward is ==> -1667.0535810965919\n",
      "Episode * 84 * Avg Reward is ==> -1666.9479653996857\n",
      "Episode * 85 * Avg Reward is ==> -1666.1664052976193\n",
      "Episode * 86 * Avg Reward is ==> -1664.3714221977075\n",
      "Episode * 87 * Avg Reward is ==> -1664.3570184646574\n",
      "Episode * 88 * Avg Reward is ==> -1664.1316708813313\n",
      "Episode * 89 * Avg Reward is ==> -1663.6715923237043\n",
      "Episode * 90 * Avg Reward is ==> -1663.7841121595804\n",
      "Episode * 91 * Avg Reward is ==> -1664.024671538399\n",
      "Episode * 92 * Avg Reward is ==> -1663.9628578954162\n",
      "Episode * 93 * Avg Reward is ==> -1664.124987453581\n",
      "Episode * 94 * Avg Reward is ==> -1664.7279174538237\n",
      "Episode * 95 * Avg Reward is ==> -1664.9113997739657\n",
      "Episode * 96 * Avg Reward is ==> -1664.8491733782932\n",
      "Episode * 97 * Avg Reward is ==> -1664.9002242789734\n",
      "Episode * 98 * Avg Reward is ==> -1665.1187912542612\n",
      "Episode * 99 * Avg Reward is ==> -1664.6501834914266\n",
      "Episode * 100 * Avg Reward is ==> -1664.8299559478112\n",
      "Episode * 101 * Avg Reward is ==> -1664.6146160091935\n",
      "Episode * 102 * Avg Reward is ==> -1664.7034964770821\n",
      "Episode * 103 * Avg Reward is ==> -1665.8747369388882\n",
      "Episode * 104 * Avg Reward is ==> -1666.1637701261914\n",
      "Episode * 105 * Avg Reward is ==> -1666.364828625846\n",
      "Episode * 106 * Avg Reward is ==> -1666.4275450744253\n",
      "Episode * 107 * Avg Reward is ==> -1665.8462427543525\n",
      "Episode * 108 * Avg Reward is ==> -1664.8483443690516\n",
      "Episode * 109 * Avg Reward is ==> -1663.5446456502752\n",
      "Episode * 110 * Avg Reward is ==> -1663.8039745931692\n",
      "Episode * 111 * Avg Reward is ==> -1663.8260134290797\n",
      "Episode * 112 * Avg Reward is ==> -1663.8736661298306\n",
      "Episode * 113 * Avg Reward is ==> -1664.8372564740496\n",
      "Episode * 114 * Avg Reward is ==> -1664.7585265138698\n",
      "Episode * 115 * Avg Reward is ==> -1665.0488007183844\n",
      "Episode * 116 * Avg Reward is ==> -1664.9861313522767\n",
      "Episode * 117 * Avg Reward is ==> -1664.7131322612584\n",
      "Episode * 118 * Avg Reward is ==> -1665.4329832842454\n",
      "Episode * 119 * Avg Reward is ==> -1665.5954052780862\n",
      "Episode * 120 * Avg Reward is ==> -1665.604715844326\n",
      "Episode * 121 * Avg Reward is ==> -1666.733525077524\n",
      "Episode * 122 * Avg Reward is ==> -1667.2163941544254\n",
      "Episode * 123 * Avg Reward is ==> -1667.671795314622\n",
      "Episode * 124 * Avg Reward is ==> -1667.760070803586\n",
      "Episode * 125 * Avg Reward is ==> -1667.8370139719495\n",
      "Episode * 126 * Avg Reward is ==> -1667.7795445900576\n",
      "Episode * 127 * Avg Reward is ==> -1667.697806661416\n",
      "Episode * 128 * Avg Reward is ==> -1667.769659670385\n",
      "Episode * 129 * Avg Reward is ==> -1667.4796511707139\n",
      "Episode * 130 * Avg Reward is ==> -1667.0696090147412\n",
      "Episode * 131 * Avg Reward is ==> -1667.1655866725341\n",
      "Episode * 132 * Avg Reward is ==> -1667.0864810977052\n",
      "Episode * 133 * Avg Reward is ==> -1666.7530191700475\n",
      "Episode * 134 * Avg Reward is ==> -1666.5379560356623\n",
      "Episode * 135 * Avg Reward is ==> -1667.6743554323866\n",
      "Episode * 136 * Avg Reward is ==> -1669.378407187307\n",
      "Episode * 137 * Avg Reward is ==> -1669.5047425618345\n",
      "Episode * 138 * Avg Reward is ==> -1670.1281679252277\n",
      "Episode * 139 * Avg Reward is ==> -1670.1942534616742\n",
      "Episode * 140 * Avg Reward is ==> -1669.2243574554968\n",
      "Episode * 141 * Avg Reward is ==> -1668.8468665061546\n",
      "Episode * 142 * Avg Reward is ==> -1668.4279680029888\n",
      "Episode * 143 * Avg Reward is ==> -1667.615026506141\n",
      "Episode * 144 * Avg Reward is ==> -1666.7156090799087\n",
      "Episode * 145 * Avg Reward is ==> -1666.2825653461678\n",
      "Episode * 146 * Avg Reward is ==> -1665.427422763255\n",
      "Episode * 147 * Avg Reward is ==> -1664.5490210042078\n",
      "Episode * 148 * Avg Reward is ==> -1664.180161451704\n",
      "Episode * 149 * Avg Reward is ==> -1664.3893404216165\n",
      "Episode * 150 * Avg Reward is ==> -1664.846570037948\n",
      "Episode * 151 * Avg Reward is ==> -1665.030554764588\n",
      "Episode * 152 * Avg Reward is ==> -1665.6389846161055\n",
      "Episode * 153 * Avg Reward is ==> -1665.4364253732222\n",
      "Episode * 154 * Avg Reward is ==> -1665.3200323188437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 155 * Avg Reward is ==> -1665.6816077859405\n",
      "Episode * 156 * Avg Reward is ==> -1665.9984911977924\n",
      "Episode * 157 * Avg Reward is ==> -1666.6667000852217\n",
      "Episode * 158 * Avg Reward is ==> -1667.8066236590862\n",
      "Episode * 159 * Avg Reward is ==> -1668.7944783408445\n",
      "Episode * 160 * Avg Reward is ==> -1668.7731914521353\n",
      "Episode * 161 * Avg Reward is ==> -1668.403843399081\n",
      "Episode * 162 * Avg Reward is ==> -1668.5896204054545\n",
      "Episode * 163 * Avg Reward is ==> -1668.0667055759338\n",
      "Episode * 164 * Avg Reward is ==> -1667.619684832689\n",
      "Episode * 165 * Avg Reward is ==> -1666.5503058222616\n",
      "Episode * 166 * Avg Reward is ==> -1666.640759948899\n",
      "Episode * 167 * Avg Reward is ==> -1666.6595008536394\n",
      "Episode * 168 * Avg Reward is ==> -1666.109834610779\n",
      "Episode * 169 * Avg Reward is ==> -1666.2068694705997\n",
      "Episode * 170 * Avg Reward is ==> -1666.4791488903204\n",
      "Episode * 171 * Avg Reward is ==> -1666.4080700930488\n",
      "Episode * 172 * Avg Reward is ==> -1666.710711214109\n",
      "Episode * 173 * Avg Reward is ==> -1666.6136258008626\n",
      "Episode * 174 * Avg Reward is ==> -1666.9856490159427\n",
      "Episode * 175 * Avg Reward is ==> -1667.3228116092107\n",
      "Episode * 176 * Avg Reward is ==> -1667.6088951813788\n",
      "Episode * 177 * Avg Reward is ==> -1667.7733675807915\n",
      "Episode * 178 * Avg Reward is ==> -1667.468884661093\n",
      "Episode * 179 * Avg Reward is ==> -1667.773785724019\n",
      "Episode * 180 * Avg Reward is ==> -1668.4266493036353\n",
      "Episode * 181 * Avg Reward is ==> -1668.611605285487\n",
      "Episode * 182 * Avg Reward is ==> -1668.1998379738322\n",
      "Episode * 183 * Avg Reward is ==> -1667.7589451886556\n",
      "Episode * 184 * Avg Reward is ==> -1667.7481756125237\n",
      "Episode * 185 * Avg Reward is ==> -1667.2575134081987\n",
      "Episode * 186 * Avg Reward is ==> -1666.0415569348866\n",
      "Episode * 187 * Avg Reward is ==> -1666.1767717670195\n",
      "Episode * 188 * Avg Reward is ==> -1666.1550364847844\n",
      "Episode * 189 * Avg Reward is ==> -1665.0130346999117\n",
      "Episode * 190 * Avg Reward is ==> -1665.5305356609765\n",
      "Episode * 191 * Avg Reward is ==> -1665.5948731151066\n",
      "Episode * 192 * Avg Reward is ==> -1665.6780204079757\n",
      "Episode * 193 * Avg Reward is ==> -1666.4109123551357\n",
      "Episode * 194 * Avg Reward is ==> -1667.2419666700998\n",
      "Episode * 195 * Avg Reward is ==> -1667.7820867890061\n",
      "Episode * 196 * Avg Reward is ==> -1668.346473596138\n",
      "Episode * 197 * Avg Reward is ==> -1668.836456103937\n",
      "Episode * 198 * Avg Reward is ==> -1669.1259157208106\n",
      "Episode * 199 * Avg Reward is ==> -1668.68338547088\n",
      "Episode * 200 * Avg Reward is ==> -1668.7837715434553\n",
      "Episode * 201 * Avg Reward is ==> -1669.0162020867276\n",
      "Episode * 202 * Avg Reward is ==> -1668.7540878608393\n",
      "Episode * 203 * Avg Reward is ==> -1669.2068392568506\n",
      "Episode * 204 * Avg Reward is ==> -1666.6789287795802\n",
      "Episode * 205 * Avg Reward is ==> -1666.300392052508\n",
      "Episode * 206 * Avg Reward is ==> -1666.1468776243762\n",
      "Episode * 207 * Avg Reward is ==> -1666.7307295357937\n",
      "Episode * 208 * Avg Reward is ==> -1666.7469960655028\n",
      "Episode * 209 * Avg Reward is ==> -1666.8398129020015\n",
      "Episode * 210 * Avg Reward is ==> -1666.919942587189\n",
      "Episode * 211 * Avg Reward is ==> -1666.8678763039766\n",
      "Episode * 212 * Avg Reward is ==> -1666.893853404345\n",
      "Episode * 213 * Avg Reward is ==> -1667.4163951855276\n",
      "Episode * 214 * Avg Reward is ==> -1667.8736761005691\n",
      "Episode * 215 * Avg Reward is ==> -1669.215349202273\n",
      "Episode * 216 * Avg Reward is ==> -1669.2453561726643\n",
      "Episode * 217 * Avg Reward is ==> -1669.5537939455953\n",
      "Episode * 218 * Avg Reward is ==> -1669.8895256717058\n",
      "Episode * 219 * Avg Reward is ==> -1670.2695733105545\n",
      "Episode * 220 * Avg Reward is ==> -1669.9297029117274\n",
      "Episode * 221 * Avg Reward is ==> -1669.5424254551162\n",
      "Episode * 222 * Avg Reward is ==> -1668.7313094363678\n",
      "Episode * 223 * Avg Reward is ==> -1668.022145073321\n",
      "Episode * 224 * Avg Reward is ==> -1667.4310358632317\n",
      "Episode * 225 * Avg Reward is ==> -1667.2925082941679\n",
      "Episode * 226 * Avg Reward is ==> -1666.7690425043286\n",
      "Episode * 227 * Avg Reward is ==> -1666.9253807706743\n",
      "Episode * 228 * Avg Reward is ==> -1667.260869999939\n",
      "Episode * 229 * Avg Reward is ==> -1666.8574730675434\n",
      "Episode * 230 * Avg Reward is ==> -1666.528456530517\n",
      "Episode * 231 * Avg Reward is ==> -1667.1482143504122\n",
      "Episode * 232 * Avg Reward is ==> -1667.5753921605824\n",
      "Episode * 233 * Avg Reward is ==> -1668.3480842972035\n",
      "Episode * 234 * Avg Reward is ==> -1667.8236433821266\n",
      "Episode * 235 * Avg Reward is ==> -1668.1068797132152\n",
      "Episode * 236 * Avg Reward is ==> -1669.047753866243\n",
      "Episode * 237 * Avg Reward is ==> -1669.059164425425\n",
      "Episode * 238 * Avg Reward is ==> -1669.397350698738\n",
      "Episode * 239 * Avg Reward is ==> -1671.1707725834713\n",
      "Episode * 240 * Avg Reward is ==> -1671.5131349661553\n",
      "Episode * 241 * Avg Reward is ==> -1671.6941755664382\n",
      "Episode * 242 * Avg Reward is ==> -1671.9485611461332\n",
      "Episode * 243 * Avg Reward is ==> -1671.6698235432586\n",
      "Episode * 244 * Avg Reward is ==> -1671.2380429769878\n",
      "Episode * 245 * Avg Reward is ==> -1671.3757719651842\n",
      "Episode * 246 * Avg Reward is ==> -1671.3009715777357\n",
      "Episode * 247 * Avg Reward is ==> -1671.055010427357\n",
      "Episode * 248 * Avg Reward is ==> -1671.3456632159127\n",
      "Episode * 249 * Avg Reward is ==> -1671.610517654376\n",
      "Episode * 250 * Avg Reward is ==> -1671.4223888578365\n",
      "Episode * 251 * Avg Reward is ==> -1671.2638380722854\n",
      "Episode * 252 * Avg Reward is ==> -1671.3896940004825\n",
      "Episode * 253 * Avg Reward is ==> -1671.2845441890977\n",
      "Episode * 254 * Avg Reward is ==> -1673.8225743387686\n",
      "Episode * 255 * Avg Reward is ==> -1673.95712367779\n",
      "Episode * 256 * Avg Reward is ==> -1674.0164598045144\n",
      "Episode * 257 * Avg Reward is ==> -1673.836585662388\n",
      "Episode * 258 * Avg Reward is ==> -1674.2831931149847\n",
      "Episode * 259 * Avg Reward is ==> -1674.267622407085\n",
      "Episode * 260 * Avg Reward is ==> -1673.943554148545\n",
      "Episode * 261 * Avg Reward is ==> -1673.6517183727365\n",
      "Episode * 262 * Avg Reward is ==> -1673.5051169947103\n",
      "Episode * 263 * Avg Reward is ==> -1672.8428176920174\n",
      "Episode * 264 * Avg Reward is ==> -1672.3085034892238\n",
      "Episode * 265 * Avg Reward is ==> -1671.9563882000307\n",
      "Episode * 266 * Avg Reward is ==> -1671.6460718556657\n",
      "Episode * 267 * Avg Reward is ==> -1670.979671352275\n",
      "Episode * 268 * Avg Reward is ==> -1670.8533139432477\n",
      "Episode * 269 * Avg Reward is ==> -1670.652281907617\n",
      "Episode * 270 * Avg Reward is ==> -1670.631574501017\n",
      "Episode * 271 * Avg Reward is ==> -1670.6091837114332\n",
      "Episode * 272 * Avg Reward is ==> -1670.9645569632464\n",
      "Episode * 273 * Avg Reward is ==> -1671.5487276944361\n",
      "Episode * 274 * Avg Reward is ==> -1671.9208227185736\n",
      "Episode * 275 * Avg Reward is ==> -1672.0404956847797\n",
      "Episode * 276 * Avg Reward is ==> -1672.5048449561855\n",
      "Episode * 277 * Avg Reward is ==> -1672.241770890826\n",
      "Episode * 278 * Avg Reward is ==> -1669.5325094369578\n",
      "Episode * 279 * Avg Reward is ==> -1669.3434854622776\n",
      "Episode * 280 * Avg Reward is ==> -1668.9758709727416\n",
      "Episode * 281 * Avg Reward is ==> -1668.5031758388513\n",
      "Episode * 282 * Avg Reward is ==> -1668.0569521440505\n",
      "Episode * 283 * Avg Reward is ==> -1667.981617825243\n",
      "Episode * 284 * Avg Reward is ==> -1668.9315522848804\n",
      "Episode * 285 * Avg Reward is ==> -1667.7655481274517\n",
      "Episode * 286 * Avg Reward is ==> -1667.8058096659288\n",
      "Episode * 287 * Avg Reward is ==> -1667.7302529825713\n",
      "Episode * 288 * Avg Reward is ==> -1667.126840050769\n",
      "Episode * 289 * Avg Reward is ==> -1666.598902123537\n",
      "Episode * 290 * Avg Reward is ==> -1666.2183889632436\n",
      "Episode * 291 * Avg Reward is ==> -1665.7913637878362\n",
      "Episode * 292 * Avg Reward is ==> -1665.429940538373\n",
      "Episode * 293 * Avg Reward is ==> -1665.1251371668523\n",
      "Episode * 294 * Avg Reward is ==> -1665.388645430046\n",
      "Episode * 295 * Avg Reward is ==> -1665.329392854493\n",
      "Episode * 296 * Avg Reward is ==> -1666.1150875489998\n",
      "Episode * 297 * Avg Reward is ==> -1666.7621427988115\n",
      "Episode * 298 * Avg Reward is ==> -1666.6833142213957\n",
      "Episode * 299 * Avg Reward is ==> -1666.6181386311\n",
      "Episode * 300 * Avg Reward is ==> -1666.4784433353138\n",
      "Episode * 301 * Avg Reward is ==> -1666.1288438209742\n",
      "Episode * 302 * Avg Reward is ==> -1666.0609219540609\n",
      "Episode * 303 * Avg Reward is ==> -1665.634298820278\n",
      "Episode * 304 * Avg Reward is ==> -1664.8142667451395\n",
      "Episode * 305 * Avg Reward is ==> -1664.4497351431235\n",
      "Episode * 306 * Avg Reward is ==> -1664.7596844198674\n",
      "Episode * 307 * Avg Reward is ==> -1664.5250496653232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 308 * Avg Reward is ==> -1664.308113229805\n",
      "Episode * 309 * Avg Reward is ==> -1664.838294165642\n",
      "Episode * 310 * Avg Reward is ==> -1665.1729359720318\n",
      "Episode * 311 * Avg Reward is ==> -1664.9437237723641\n",
      "Episode * 312 * Avg Reward is ==> -1664.439998398052\n",
      "Episode * 313 * Avg Reward is ==> -1664.5995486569436\n",
      "Episode * 314 * Avg Reward is ==> -1665.0563845887314\n",
      "Episode * 315 * Avg Reward is ==> -1664.8194900035899\n",
      "Episode * 316 * Avg Reward is ==> -1664.8037462805423\n",
      "Episode * 317 * Avg Reward is ==> -1665.4507851457338\n",
      "Episode * 318 * Avg Reward is ==> -1664.4738069303476\n",
      "Episode * 319 * Avg Reward is ==> -1664.0187156109748\n",
      "Episode * 320 * Avg Reward is ==> -1663.1266612334307\n",
      "Episode * 321 * Avg Reward is ==> -1662.5137885123902\n",
      "Episode * 322 * Avg Reward is ==> -1661.3954268994185\n",
      "Episode * 323 * Avg Reward is ==> -1661.1841356766936\n",
      "Episode * 324 * Avg Reward is ==> -1661.1536939279586\n",
      "Episode * 325 * Avg Reward is ==> -1661.4498596956091\n",
      "Episode * 326 * Avg Reward is ==> -1661.4306588798288\n",
      "Episode * 327 * Avg Reward is ==> -1661.6655457417246\n",
      "Episode * 328 * Avg Reward is ==> -1664.535755850929\n",
      "Episode * 329 * Avg Reward is ==> -1664.9011693982125\n",
      "Episode * 330 * Avg Reward is ==> -1665.1978690234773\n",
      "Episode * 331 * Avg Reward is ==> -1664.546883872962\n",
      "Episode * 332 * Avg Reward is ==> -1664.9896332795258\n",
      "Episode * 333 * Avg Reward is ==> -1665.2177374285748\n",
      "Episode * 334 * Avg Reward is ==> -1665.064918695635\n",
      "Episode * 335 * Avg Reward is ==> -1666.5447993140117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-4c661c0007d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mepisodic_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-914607f0a4e6>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-914607f0a4e6>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, state_batch, action_batch, reward_batch, next_state_batch)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mactor_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         actor_optimizer.apply_gradients(\n\u001b[0;32m     45\u001b[0m             \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py\u001b[0m in \u001b[0;36m_ConcatGradV2\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_ConcatGradV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m   return _ConcatGradHelper(\n\u001b[1;32m--> 229\u001b[1;33m       op, grad, start_value_index=0, end_value_index=-1, dim_index=-1)\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py\u001b[0m in \u001b[0;36m_ConcatGradHelper\u001b[1;34m(op, grad, start_value_index, end_value_index, dim_index)\u001b[0m\n\u001b[0;32m    116\u001b[0m       \u001b[1;31m# in concat implementation to be within the allowed [-rank, rank) range.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m       non_neg_concat_dim = (\n\u001b[1;32m--> 118\u001b[1;33m           concat_dim._numpy().item(0) % input_values[0]._rank())  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    119\u001b[0m       \u001b[1;31m# All inputs are guaranteed to be EagerTensors in eager mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m       sizes = pywrap_tensorflow.TFE_Py_TensorShapeSlice(input_values,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ep_reward_list = []\n",
    "\n",
    "avg_reward_list = []\n",
    "\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "    \n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_reward = np.mean(ep_reward_list[-50:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
